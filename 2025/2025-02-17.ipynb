{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class notebook: 2025-02-17\n",
    "\n",
    "In this notebook, we will look at a few different regression techniques. We demonstrate this on a simulated supernova distance modulus-redshift dataset.  \n",
    "\n",
    "This notebook is intended to support Chapter 8 of the textbook, and material is taken from the following script (from astroML):\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter8/astroml_chapter8_Regression.ipynb\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter8/fig_regression_mu_z.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter8/fig_rbf_ridge_mu_z.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter8/fig_gp_mu_z.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first create some fake data with error bars from a true underlying model\n",
    "\n",
    "from astropy.cosmology import LambdaCDM\n",
    "from astroML.datasets import generate_mu_z\n",
    "# this calls an inbuild-function here \n",
    "# https://github.com/astroML/astroML/blob/main/astroML/datasets/generated.py\n",
    "# the error bars have the form dmu = dmu_0 + dmu_1 * mu\n",
    "\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0, dmu_0=0.1, dmu_1=0.02)\n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = cosmo.distmod(z)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We first try a linear regression using the matrix formalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mu_sample.reshape(100,1)\n",
    "M = np.zeros((100,2))\n",
    "M[:,0] = 1\n",
    "M[:,1] = z_sample.copy()\n",
    "C = np.diag(np.ones(100)*dmu**2)\n",
    "# C = np.diag(np.ones(100))\n",
    "# uncomment to get homoscedastic errorr bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (M^TC^-1M)^-1\n",
    "Theta1 = np.linalg.inv(np.dot(np.dot(M.T, np.linalg.inv(C)), M))\n",
    "\n",
    "# M^TC^-1Y\n",
    "Theta2 = np.dot(np.dot(M.T, np.linalg.inv(C)), Y)\n",
    "\n",
    "# Theta\n",
    "Theta = np.dot(Theta1, Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least square best-fit parameters\n",
    "print(Theta)\n",
    "\n",
    "# covariance between parameters\n",
    "print(Theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "plt.plot(z, Theta[0]+Theta[1]*z, ls='--', color='blue')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We next move onto the inbuilt functions in sklearn, still linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LinearRegression_sk \n",
    "# read about linear models in sklearn here: \n",
    "# https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "linear_sk = LinearRegression_sk()\n",
    "\n",
    "# this is doing the \"training\"\n",
    "linear_sk.fit(z_sample[:,None], mu_sample)\n",
    "\n",
    "mu_fit_sk = linear_sk.predict(z[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "plt.plot(z, Theta[0]+Theta[1]*z, ls='--', color='b')\n",
    "# note that in the above we have not considered errors, which is resulting in \n",
    "# some difference compared with the matrix formalism\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# astroML is wrapping sklearn to give some additional functions\n",
    "# https://github.com/astroML/astroML/blob/main/astroML/linear_model/linear_regression.py\n",
    "# https://github.com/astroML/astroML/blob/main/astroML/linear_model/linear_regression_errors.py\n",
    "\n",
    "from astroML.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "\n",
    "# this is doing the \"training\"\n",
    "# now including the errors\n",
    "linear.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit = linear.predict(z[:, None])\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_fit, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "plt.plot(z, Theta[0]+Theta[1]*z, ls='--', color='b')\n",
    "\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next try basis function regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poynomials\n",
    "\n",
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "# 2nd degree polynomial regression\n",
    "polynomial = PolynomialRegression(degree=2)\n",
    "# this is doing the \"training\"\n",
    "polynomial.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_poly = polynomial.predict(z[:, None])\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_poly, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian basis\n",
    "\n",
    "from astroML.linear_model import BasisFunctionRegression\n",
    "\n",
    "# Define our number of Gaussians\n",
    "nGaussians = 10\n",
    "basis_mu = np.linspace(0, 2, nGaussians)[:, None]\n",
    "basis_sigma = 3 * (basis_mu[1] - basis_mu[0])\n",
    "\n",
    "gauss_basis = BasisFunctionRegression('gaussian', mu=basis_mu, sigma=basis_sigma)\n",
    "# this is doing the \"training\"\n",
    "gauss_basis.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_gauss = gauss_basis.predict(z[:, None])\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_gauss, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try kernal regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian kernal (Nadaraya-Watson)\n",
    "\n",
    "from astroML.linear_model import NadarayaWatson\n",
    "\n",
    "# Define kernal\n",
    "\n",
    "NWkernal = NadarayaWatson('gaussian', h=0.1)\n",
    "# this is doing the \"training\"\n",
    "NWkernal.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_NWkernal = NWkernal.predict(z[:, None])\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_NWkernal, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at some regularized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no regression, Ridge regression (L2), Lasso regression (L1)\n",
    "\n",
    "regularization = ['none', 'l2', 'l1']\n",
    "kwargs = [dict(), dict(alpha=0.005), dict(alpha=0.001)]\n",
    "labels = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "color= ['teal', 'orange', 'navy']\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "# Manually convert data to a gaussian basis\n",
    "# note that we're ignoring errors here, for the sake of example.\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "centers = np.linspace(0, 1.8, 100)\n",
    "widths = 0.2\n",
    "X = gaussian_basis(z_sample[:, np.newaxis], centers, widths)\n",
    "\n",
    "for i in range(3):\n",
    "    clf = LinearRegression(regularization=regularization[i],\n",
    "                           fit_intercept=True, kwds=kwargs[i])\n",
    "    # this is doing the \"training\"\n",
    "    clf.fit(X, mu_sample, dmu)\n",
    "    fit = clf.predict(gaussian_basis(z[:, None], centers, widths))\n",
    "\n",
    "    ax.plot(z, fit, '-k', color=color[i], label=labels[i])\n",
    "\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "# this is what converting to a Gaussian basis means\n",
    "# each z point is now mapped to a Gaussian\n",
    "# this makes the problem highly correlated, thus the regularization is needed\n",
    "plt.figure()\n",
    "print(z_sample[0], z_sample[1])\n",
    "plt.plot(X[0])\n",
    "plt.plot(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Gaussian process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read about Gaussian process in sklearn here: \n",
    "# https://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu ** 2)\n",
    "\n",
    "# this is doing the \"training\"\n",
    "gp.fit(z_sample[:, None], mu_sample)\n",
    "\n",
    "# mesh the input space for evaluations of the real function, the prediction and its MSE\n",
    "z_fit = np.linspace(0, 2, 1000)\n",
    "y_pred, sigma = gp.predict(z_fit[:, None], return_std=True)\n",
    "print(sigma.shape)\n",
    "\n",
    "# Plot the gaussian process\n",
    "#  gaussian process allows computation of the error at each point\n",
    "#  so we will show this as a shaded region\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_true, '--k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', markersize=6)\n",
    "ax.plot(z_fit, y_pred, '-k')\n",
    "ax.fill_between(z_fit, y_pred - 2.0 * sigma, y_pred + 2.0 * sigma,\n",
    "                alpha=0.2, color='b', label='95% confidence interval')\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(36, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now nonlinear regression\n",
    "\n",
    "ie. \"fitting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.utils.decorators import pickle_results\n",
    "\n",
    "# we want to use tighter errors here, just for illustration\n",
    "# z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0, dmu_0=0.005, dmu_1=0.005)\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, z0=0.3,\n",
    "                                         dmu_0=0.05, dmu_1=0.004,\n",
    "                                         random_state=0)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# define a log likelihood in terms of the parameters\n",
    "#  beta = [omegaM, omegaL]\n",
    "def compute_logL(beta):\n",
    "    cosmo = LambdaCDM(H0=71, Om0=beta[0], Ode0=beta[1], Tcmb0=0)\n",
    "    mu_pred = cosmo.distmod(z_sample).value\n",
    "    return - np.sum(0.5 * ((mu_sample - mu_pred) / dmu) ** 2)\n",
    "\n",
    "# Define a function to compute (and save to file) the log-likelihood\n",
    "# @pickle_results('mu_z_nonlinear.pkl')\n",
    "def compute_mu_z_nonlinear(Nbins=200):\n",
    "    omegaM = np.linspace(0.05, 0.75, Nbins)\n",
    "    omegaL = np.linspace(0.4, 1.1, Nbins)\n",
    "\n",
    "    logL = np.empty((Nbins, Nbins))\n",
    "\n",
    "    for i in range(len(omegaM)):\n",
    "        for j in range(len(omegaL)):\n",
    "            logL[i, j] = compute_logL([omegaM[i], omegaL[j]])\n",
    "\n",
    "    return omegaM, omegaL, logL\n",
    "\n",
    "\n",
    "omegaM, omegaL, res = compute_mu_z_nonlinear()\n",
    "res -= np.max(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# left plot: the data and best-fit\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "# getting best-fit\n",
    "whr = np.where(res == np.max(res))\n",
    "omegaM_best = omegaM[whr[0][0]]\n",
    "omegaL_best = omegaL[whr[1][0]]\n",
    "cosmo = LambdaCDM(H0=71, Om0=omegaM_best, Ode0=omegaL_best, Tcmb0=0)\n",
    "\n",
    "z_fit = np.linspace(0.04, 2, 100)\n",
    "mu_fit = cosmo.distmod(z_fit).value\n",
    "\n",
    "ax.plot(z_fit, mu_fit, '-k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray')\n",
    "\n",
    "ax.set_xlim(0, 1.8)\n",
    "ax.set_ylim(36, 46)\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.text(0.04, 0.96, \"%i observations\" % len(z_sample),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "# right plot: the likelihood\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "ax.contour(omegaM, omegaL, convert_to_stdev(res.T),\n",
    "           levels=(0.683, 0.955, 0.997),\n",
    "           colors='k')\n",
    "\n",
    "ax.plot([0, 1], [1, 0], '--k')\n",
    "ax.plot([0, 1], [0.73, 0.73], ':k')\n",
    "ax.plot([0.27, 0.27], [0, 2], ':k')\n",
    "\n",
    "ax.set_xlim(0.05, 0.75)\n",
    "ax.set_ylim(0.4, 1.1)\n",
    "\n",
    "ax.set_xlabel(r'$\\Omega_M$')\n",
    "ax.set_ylabel(r'$\\Omega_\\Lambda$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
