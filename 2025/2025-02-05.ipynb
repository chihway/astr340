{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class notebook: 2024-02-05\n",
    "\n",
    "\n",
    "In this notebook, we demonstrate how to run a simple MCMC for Bayesian parameter estimation. We then look at how we can do model comparison with the MCMC chain, and an example of Hierarchical Bayesian computation. \n",
    "\n",
    "This notebook is intended to support Chapter 5.7-5.11 of the textbook, and material is taken from the following scripts (from astroML):\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter5/fig_cauchy_mcmc.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter5/fig_model_comparison_mcmc.py\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter5/astroml_chapter5_Hierarchical_Bayes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic MCMC: Cauchy distribution\n",
    "\n",
    "Here we look at how to use Markov chain monte carlo (MCMC) to estimate the posterior pdf for parameters describing the Cauchy distribution. \n",
    "\n",
    "The logarithm of the posterior pdf is\n",
    "\n",
    "$$L_p \\equiv ln[p(\\mu,\\gamma|{x_i},I)] = constant + (N-1)ln\\gamma - \\sum^N_{i=1}ln[\\gamma^2 + (x_i-\\mu)^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import cauchy\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the log likelihood\n",
    "def cauchy_logL(xi, gamma, mu):\n",
    "    \"\"\"Equation 5.74: cauchy likelihood\"\"\"\n",
    "    xi = np.asarray(xi)\n",
    "    n = xi.size\n",
    "    shape = np.broadcast(gamma, mu).shape\n",
    "\n",
    "    xi = xi.reshape(xi.shape + tuple([1 for s in shape]))\n",
    "\n",
    "    return ((n - 1) * np.log(gamma)\n",
    "            - np.sum(np.log(gamma ** 2 + (xi - mu) ** 2), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the sample from a Cauchy distribution\n",
    "np.random.seed(44)\n",
    "mu_0 = 0\n",
    "gamma_0 = 2\n",
    "N = 100\n",
    "xi = cauchy(mu_0, gamma_0).rvs(N)\n",
    "\n",
    "plt.hist(xi)\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and run MCMC:\n",
    "with pm.Model():\n",
    "    # uniform prior in mu\n",
    "    mu = pm.Uniform('mu', -5, 5)\n",
    "    # uniform prior in ln(gamma)\n",
    "    log_gamma = pm.Uniform('log_gamma', -10, 10)\n",
    "\n",
    "    # set up our observed variable x\n",
    "    # use inbuilt Cauchy likelihood from pymc3\n",
    "    # a list of existing distributions can be found here \n",
    "    # https://github.com/pymc-devs/pymc/blob/main/pymc/distributions/continuous.py\n",
    "    x = pm.Cauchy('x', mu, np.exp(log_gamma), observed=xi)\n",
    "\n",
    "    trace = pm.sample(draws=12000, tune=1000, cores=1)\n",
    "    # tune: burn-in\n",
    "    # this is using the No-U-Turn Sampler (NUTS) algorithm (Hoffman & Gelman, 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute histogram of results to plot below\n",
    "L_MCMC, mu_bins, gamma_bins = np.histogram2d(trace['mu'],\n",
    "                                             np.exp(trace['log_gamma']),\n",
    "                                             bins=(np.linspace(-5, 5, 41),\n",
    "                                                   np.linspace(0, 5, 41)))\n",
    "L_MCMC[L_MCMC == 0] = 1E-16  # prevents zero-division errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute likelihood analytically for comparison\n",
    "mu = np.linspace(-5, 5, 70)\n",
    "gamma = np.linspace(0.1, 5, 70)\n",
    "logL = cauchy_logL(xi, gamma[:, np.newaxis], mu)\n",
    "logL -= logL.max()\n",
    "\n",
    "# Compute marginalized distributions \n",
    "p_mu = np.exp(logL).sum(0)\n",
    "p_mu /= p_mu.sum() * (mu[1] - mu[0])\n",
    "\n",
    "p_gamma = np.exp(logL).sum(1)\n",
    "p_gamma /= p_gamma.sum() * (gamma[1] - gamma[0])\n",
    "\n",
    "hist_mu, bins_mu = np.histogram(trace['mu'], bins=mu_bins, density=True)\n",
    "hist_gamma, bins_gamma = np.histogram(np.exp(trace['log_gamma']),\n",
    "                                      bins=gamma_bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# first axis: likelihood contours\n",
    "ax1 = fig.add_axes((0.4, 0.4, 0.55, 0.55))\n",
    "ax1.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax1.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax1.contour(mu, gamma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='b', linestyles='dashed')\n",
    "\n",
    "ax1.contour(0.5 * (mu_bins[:-1] + mu_bins[1:]),\n",
    "            0.5 * (gamma_bins[:-1] + gamma_bins[1:]),\n",
    "            convert_to_stdev(np.log(L_MCMC.T)),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "# second axis: marginalized over mu\n",
    "ax2 = fig.add_axes((0.1, 0.4, 0.29, 0.55))\n",
    "ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax2.plot(hist_gamma, 0.5 * (bins_gamma[1:] + bins_gamma[:-1]\n",
    "                            - bins_gamma[1] + bins_gamma[0]),\n",
    "         '-k', drawstyle='steps')\n",
    "ax2.plot(p_gamma, gamma, '--b')\n",
    "ax2.set_ylabel(r'$\\gamma$')\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "# third axis: marginalized over gamma\n",
    "ax3 = fig.add_axes((0.4, 0.1, 0.55, 0.29))\n",
    "ax3.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax3.plot(0.5 * (bins_mu[1:] + bins_mu[:-1]), hist_mu,\n",
    "         '-k', drawstyle='steps-mid')\n",
    "ax3.plot(mu, p_mu, '--b')\n",
    "ax3.set_xlabel(r'$\\mu$')\n",
    "plt.xlim(-5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MCMC for model comparison\n",
    "\n",
    "We have some data points and we want to compare the odds ratio of this data being drawn from two models: a single Gaussian or two Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "from sklearn.neighbors import BallTree\n",
    "import theano.tensor as tt\n",
    "\n",
    "from astroML.density_estimation import GaussianMixture1D\n",
    "from astroML.plotting import plot_mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data from 2 Gaussians\n",
    "mu1_in = 0\n",
    "sigma1_in = 0.3\n",
    "mu2_in = 1\n",
    "sigma2_in = 1\n",
    "ratio_in = 1.5\n",
    "N = 200\n",
    "\n",
    "np.random.seed(10)\n",
    "gm = GaussianMixture1D([mu1_in, mu2_in],\n",
    "                       [sigma1_in, sigma2_in],\n",
    "                       [ratio_in, 1])\n",
    "x_sample = gm.sample(N)[0]\n",
    "\n",
    "plt.hist(x_sample)\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data to the two models, running an MCMC for each of them\n",
    "\n",
    "# Single Gaussian, 2 parameters: (mu, sigma)\n",
    "with pm.Model() as model1:\n",
    "   \n",
    "    # priors\n",
    "    M1_mu = pm.Uniform('M1_mu', -5, 5)\n",
    "    M1_log_sigma = pm.Uniform('M1_log_sigma', -10, 10)\n",
    "    \n",
    "    # posterior\n",
    "    M1 = pm.Normal('M1', mu=M1_mu, sd=np.exp(M1_log_sigma), observed=x_sample)\n",
    "    trace1 = pm.sample(draws=2500, tune=100)\n",
    "\n",
    "# Mixture of two Gaussians: 5 parameters: (mu1, mu2, sigma1, sigma2, ratio)\n",
    "with pm.Model() as model2:\n",
    "    \n",
    "    # priors\n",
    "    M2_mu1 = pm.Uniform('M2_mu1', -5, 5)\n",
    "    M2_mu2 = pm.Uniform('M2_mu2', -5, 5)\n",
    "\n",
    "    M2_log_sigma1 = pm.Uniform('M2_log_sigma1', -10, 10)\n",
    "    M2_log_sigma2 = pm.Uniform('M2_log_sigma2', -10, 10)\n",
    "\n",
    "    ratio = pm.Uniform('ratio', 1E-3, 1E3)\n",
    "\n",
    "    w1 = ratio / (1 + ratio)\n",
    "    w2 = 1 - w1\n",
    "\n",
    "    # posterior\n",
    "    y = pm.NormalMixture('doublegauss', w=tt.stack([w1, w2]),\n",
    "                         mu=tt.stack([M2_mu1, M2_mu2]),\n",
    "                         sd=tt.stack([np.exp(M2_log_sigma1),\n",
    "                                      np.exp(M2_log_sigma2)]),\n",
    "                         observed=x_sample)\n",
    "\n",
    "    trace2 = pm.sample(draws=2500, tune=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.NormalMixture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_bayes_factor(trace, r=0.05, return_list=False):\n",
    "    \"\"\"Estimate the bayes factor using the local density of points\"\"\"\n",
    "\n",
    "    # Convert traces to a numpy array, ignore the intervals\n",
    "    trace_arr = np.array([trace[i] for i in trace.varnames if \"_interval__\" not in i])\n",
    "    trace_t = trace_arr.T\n",
    "     \n",
    "\n",
    "    # compute volume of a D-dimensional sphere of radius r\n",
    "    Vr = np.pi ** (0.5 * D) / gamma(0.5 * D + 1) * (r ** D)\n",
    "\n",
    "    # use neighbor count within r as a density estimator\n",
    "    bt = BallTree(trace_t)\n",
    "    count = bt.query_radius(trace_t, r=r, count_only=True)\n",
    "\n",
    "    # log(N*p(theta)/rho(theta))\n",
    "    BF = trace.model_logp + np.log(N_iter) + np.log(Vr) - np.log(count)\n",
    "\n",
    "    if return_list:\n",
    "        return BF\n",
    "    else:\n",
    "        p25, p50, p75 = np.percentile(BF, [25, 50, 75])\n",
    "        return p50, 0.7413 * (p75 - p25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Odds ratio with density estimation technique\n",
    "\n",
    "BF1, dBF1 = estimate_bayes_factor(trace1, r=0.05)\n",
    "BF2, dBF2 = estimate_bayes_factor(trace2, r=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BF1, BF2)\n",
    "print('Baysian Odds ratio 0_21', np.exp(BF2-BF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "labels = [r'$\\mu_1$',\n",
    "          r'$\\mu_2$',\n",
    "          r'$\\sigma_1$',\n",
    "          r'$\\sigma_2$',\n",
    "          r'${\\rm ratio}$']\n",
    "\n",
    "true_values = [mu1_in,\n",
    "               mu2_in,\n",
    "               sigma1_in,\n",
    "               sigma2_in,\n",
    "               ratio_in]\n",
    "\n",
    "limits = [(-0.18, 0.18),\n",
    "          (0.5, 1.6),\n",
    "          (0.12, 0.45),\n",
    "          (0.76, 1.3),\n",
    "          (0.3, 2.5)]\n",
    "\n",
    "# We assume mu1 < mu2, but the results may be switched\n",
    "# due to the symmetry of the problem.  If so, switch back\n",
    "if np.median(trace2['M2_mu1']) < np.median(trace2['M2_mu2']):\n",
    "    trace2_for_plot = [np.exp(trace2[i]) if 'log_sigma' in i else trace2[i] for i in\n",
    "                       ['M2_mu1', 'M2_mu2', 'M2_log_sigma1', 'M2_log_sigma2', 'ratio']]\n",
    "else:\n",
    "    trace2_for_plot = [np.exp(trace2[i]) if 'log_sigma' in i else trace2[i] for i in\n",
    "                       ['M2_mu2', 'M2_mu1', 'M2_log_sigma2', 'M2_log_sigma1', 'ratio']]\n",
    "\n",
    "# Plot the simple 2-component model\n",
    "ax, = plot_mcmc([trace1['M1_mu'], np.exp(trace1['M1_log_sigma'])],\n",
    "                fig=fig, bounds=[0.6, 0.6, 0.95, 0.95],\n",
    "                limits=[(0.3, 0.65), (0.75, 1.05)],\n",
    "                labels=[r'$\\mu$', r'$\\sigma$'], colors='k')\n",
    "\n",
    "ax.text(0.05, 0.95, \"Single Gaussian fit\", va='top', ha='left',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# Plot the 5-component model\n",
    "ax_list = plot_mcmc(trace2_for_plot, limits=limits, labels=labels,\n",
    "                    true_values=true_values, fig=fig,\n",
    "                    bounds=(0.12, 0.12, 0.95, 0.95),\n",
    "                    colors='k')\n",
    "for ax in ax_list:\n",
    "    for axis in [ax.xaxis, ax.yaxis]:\n",
    "        axis.set_major_locator(plt.MaxNLocator(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Bayesian modeling\n",
    "\n",
    "We assume that we measure the radial velocity of N stars in a star cluster. We know the measurement error, and the underlying assumption is that there is an intinsic mean and spread of the veolcity of the stars in this cluster even without noise. \n",
    "\n",
    "We can assume that the true radial velocity of a single object is drawn from a Gaussian described by $\\mu$ (systemic velocity) and $\\sigma$ (velocity dispersion). But we don't know them and thus they need to be estimated from data.\n",
    "\n",
    "In case of many objects, they will collectively constrain $\\mu$ and $\\sigma$. Many independent measurement sets (here a set of one measurement per object) that share same priors constrain them better together than can any single measurement alone. This is the key idea of Hierarchical Bayes modeling.\n",
    "\n",
    "In hierarchical, or multilevel, Bayesian analysis a prior distribution depends on unknown variables, the hyperparameters, that describe the group (population) level probabilistic model. Their priors, called hyperpriors, resemble the priors in simple (single-level) Bayesian models.\n",
    "\n",
    "Here, $\\mu$ and $\\sigma$ are priors, and their corresponding prior distributions (assumed flat below), are hyperpriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussgauss_logL(xi, ei, mu, sigma):\n",
    "    \"\"\"Equation 5.63: gaussian likelihood with gaussian errors\"\"\"\n",
    "    ndim = len(np.broadcast(sigma, mu).shape)\n",
    "\n",
    "    xi = xi.reshape(xi.shape + tuple(ndim * [1]))\n",
    "    ei = ei.reshape(ei.shape + tuple(ndim * [1]))\n",
    "\n",
    "    s2_e2 = sigma ** 2 + ei ** 2\n",
    "    return -0.5 * np.sum(np.log(s2_e2) + (xi - mu) ** 2 / s2_e2, 0)\n",
    "\n",
    "def getExpStD(x, p):\n",
    "    \"\"\"given p(x), compute expectation value and std. dev.\"\"\"\n",
    "    Ex = np.sum(x * p) / np.sum(p)\n",
    "    Sx = np.sqrt(np.sum((x - Ex) ** 2 * p) /  np.sum(p))\n",
    "    return Ex, Sx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)    # for repeatability\n",
    "\n",
    "N = 10               # number of measured stars, known\n",
    "mu_true = -50.0      # km/s, true systemic velocity, unknown\n",
    "sigma_true = 20.0    # km/s, true velocity dispersion, unknown\n",
    "ei = 10 + 40 * np.random.random(N)   # heteroscedastic errors, known\n",
    "\n",
    "# generate measurements\n",
    "xi = np.random.normal(mu_true, np.sqrt(sigma_true ** 2 + ei ** 2))\n",
    "wi = 1 / ei ** 2 / np.sum(1 / ei ** 2)\n",
    "\n",
    "# weighted mean\n",
    "wmean = np.sum(wi * xi)\n",
    "\n",
    "# uncertainty of weighted mean\n",
    "wmeane = 1 / np.sqrt(np.sum(1 / ei ** 2))\n",
    "\n",
    "# other stats\n",
    "medvel = np.median(xi)\n",
    "meanvel = np.mean(xi)\n",
    "velstd = np.std(xi)\n",
    "\n",
    "print('measured velocities', xi)\n",
    "print('measurement errors', ei)\n",
    "print('weighted mean', wmean)\n",
    "print('uncertainty of weighted mean', wmeane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.linspace(0.01, 120, 70)\n",
    "mu = np.linspace(-150, 50, 70)\n",
    "\n",
    "logL = gaussgauss_logL(xi, ei, mu, sigma[:, np.newaxis])\n",
    "logL -= logL.max()\n",
    "L = np.exp(logL)\n",
    "\n",
    "# integrate L to get marginal prob. distributions\n",
    "p_sigma = L.sum(1)\n",
    "p_sigma /= (sigma[1] - sigma[0]) * p_sigma.sum()\n",
    "\n",
    "p_mu = L.sum(0)\n",
    "p_mu /= (mu[1] - mu[0]) * p_mu.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.24,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "fig.add_axes((0.58, 0.55, 0.30, 0.40))\n",
    "\n",
    "plt.imshow(logL, origin='lower',\n",
    "           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
    "           cmap=plt.cm.binary,\n",
    "           aspect='auto')\n",
    "plt.colorbar().set_label(r'$\\log(L)$')\n",
    "plt.clim(-5, 0)\n",
    "\n",
    "plt.contour(mu, sigma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "plt.xlabel(r'${\\rm systemic \\, velocity \\, } v_s \\, {\\rm (km/s)}$')\n",
    "plt.ylabel(r'${\\rm intrinsic \\, vel. \\, dispersion \\,} \\sigma \\, {\\rm (km/s)}$')\n",
    "plt.xlim(-150, 50.0)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# plot true values\n",
    "plt.plot([mu_true, mu_true], [0, 100.0], ':r', lw=1)\n",
    "plt.plot([-200, 200.0], [sigma_true, sigma_true], ':r', lw=1)\n",
    "\n",
    "# second axis: marginalized over mu\n",
    "ax2 = fig.add_axes((0.17, 0.1, 0.3, 0.30))\n",
    "ax2.plot(mu, p_mu, '-k', label='')\n",
    "ax2.set_xlabel(r'$v_s$ (km/s)')\n",
    "ax2.set_ylabel(r'$p(v_s)$')\n",
    "ax2.set_xlim(-100, 0.0)\n",
    "ax2.set_ylim(0, 0.04)\n",
    "\n",
    "# mark expectation value for radial velocity\n",
    "Ev, Sv = getExpStD(mu, p_mu)\n",
    "plt.plot([Ev, Ev], [0, 100.0], 'g', lw=1)\n",
    "# mark true systemic velocity and weighted mean of data\n",
    "plt.plot([mu_true, mu_true], [0, 100.0], ':r', lw=1)\n",
    "plt.plot([wmean, wmean], [0, 100.0], '--b', lw=1)\n",
    "\n",
    "# plot the marginalized distribution for sigma\n",
    "ax3 = fig.add_axes((0.58, 0.1, 0.3, 0.30))\n",
    "ax3.plot(sigma, p_sigma, '-k', label='')\n",
    "ax3.set_xlabel(r'$\\sigma$ (km/s)')\n",
    "ax3.set_ylabel(r'$p(\\sigma)$')\n",
    "ax3.set_xlim(0, 100.0)\n",
    "ax3.set_ylim(0, 0.05)\n",
    "plt.plot([sigma_true, sigma_true], [0, 100.0], ':r', lw=1)\n",
    "Ed, Sd = getExpStD(sigma, p_sigma)\n",
    "plt.plot([Ed, Ed], [0, 100.0], 'g', lw=1)\n",
    "\n",
    "# plot data\n",
    "ax4 = fig.add_axes((0.17, 0.55, 0.3, 0.40))\n",
    "ax4.set_xlabel(r'$v_{obs}$ (km/s)')\n",
    "ax4.set_ylabel(r'measurement index')\n",
    "ax4.set_xlim(-150, 50)\n",
    "ax4.set_ylim(0, 11)\n",
    "# mark +-error ranges\n",
    "for i in range(0, N):\n",
    "    xL = xi[i] - ei[i]\n",
    "    xR = xi[i] + ei[i]\n",
    "    plt.plot([xL, xR], [i + 1, i + 1], 'b', lw=2)\n",
    "# mark true systemic velocity and weighted mean of data\n",
    "plt.plot([wmean, wmean], [0, 100.0], '--b', lw=1)\n",
    "plt.plot([mu_true, mu_true], [0, 100.0], ':r', lw=1)\n",
    "\n",
    "# mark posterior range for each star\n",
    "mup = Ev\n",
    "sigp = Ed\n",
    "for i in range(0, N):\n",
    "    sig0 = 1 / np.sqrt(1 / sigp ** 2 + 1 / ei[i] ** 2)\n",
    "    mu0 = (mup / sigp ** 2 + xi[i] / ei[i] ** 2) * (sig0 ** 2)\n",
    "    xL = mu0 - sig0\n",
    "    xR = mu0 + sig0\n",
    "    plt.plot([xL, xR], [i + 0.7, i + 0.7], 'g', lw=1)\n",
    "\n",
    "# and expectation value for systemic velocity\n",
    "plt.plot([mup, mup], [0, 100.0], 'g', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, the cluster velocity dispersion is astrophysically the most interesting quantity, while the posterior constraints on radial velocities for individual stars are less important.\n",
    "\n",
    "However, in other settings described by the same mathematical model the posterior pdfs for the individually measured quantities (i.e., the true radial velocity for each star in the above example) may be of more interest than the priors. In such cases the posterior pdf is marginalized over the priors.\n",
    "\n",
    "An approximate shortcut approach is to fix the priors at their most probable or expectation values. This method is known as empirical Bayes.\n",
    "\n",
    "Note that the posterior estimates of true velocities are biased relative to ML estimates, and have smaller uncertainies (hence the name shrinkage) than their measurement uncertainties!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
