{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class notebook: 2025-01-22\n",
    "\n",
    "In this notebook, we will look at some common usages of classical statistical inference. We will first look at common ways we empirically estimate error bars, and then look at hypothesis testing and ways to compare distributions. \n",
    "\n",
    "This notebook is intended to support Chapter 4.5-4.9 of the textbook, and material is taken from the following scripts (from astroML):\n",
    "\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Confidence_estimates.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Hypothesis_testing.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Comparison_of_distributions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''\n",
    "<script>\n",
    "function toggleCell(cellIndex) {\n",
    "    var cell = Jupyter.notebook.get_selected_cell();\n",
    "    var element = cell.element.find('.inner_cell');\n",
    "    element.toggle();\n",
    "}\n",
    "</script>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence estimation: Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from astroML.resample import bootstrap\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "m = 1000  # number of points\n",
    "n = 10000  # number of bootstraps\n",
    "\n",
    "# sample values from a normal distribution\n",
    "np.random.seed(123)\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "# Compute bootstrap resamplings of data\n",
    "mu1_bootstrap = bootstrap(data, n,  np.std, kwargs=dict(axis=1, ddof=1))\n",
    "mu2_bootstrap = bootstrap(data, n, sigmaG, kwargs=dict(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the theoretical expectations for the two distributions\n",
    "x = np.linspace(0.8, 1.2, 1000)\n",
    "\n",
    "# error on the estimation of sigma from bootstrap\n",
    "sigma1 = 1. / np.sqrt(2 * (m - 1))\n",
    "pdf1 = norm(1, sigma1).pdf(x)\n",
    "\n",
    "# error on the estimation of sigmaG from bootstrap\n",
    "sigma2 = 1.06 / np.sqrt(m) \n",
    "pdf2 = norm(1, sigma2).pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.hist(mu1_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std. dev.)}$')\n",
    "ax.plot(x, pdf1, color='gray')\n",
    "\n",
    "ax.hist(mu2_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$')\n",
    "ax.plot(x, pdf2, color='gray')\n",
    "\n",
    "ax.set_xlim(0.82, 1.18)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$', fontsize = 16)\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$', fontsize = 16)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "<h3>Exercise</h3>\n",
    "<hr>\n",
    "If you want to code this up from scratch, how would you do it?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_std(data, n):\n",
    "    N = len(data)\n",
    "    resample = []\n",
    "    for i in range(n):\n",
    "        choices = np.random.choice(data, size=N, replace=True)\n",
    "        resample.append(np.std(choices))\n",
    "        \n",
    "    return resample\n",
    "\n",
    "# y = bootstrap_std(data, n)\n",
    "# y = np.array(y)\n",
    "# plt.hist(y, bins=50)\n",
    "# plt.xlim(0.82, 1.18)\n",
    "\n",
    "# print(np.mean(y))\n",
    "# print(np.mean(mu1_bootstrap))\n",
    "\n",
    "HTML('''\n",
    "<button onclick=\"toggleCell()\">Show me the answer</button>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence estimation: Jackknife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.resample import jackknife\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "np.random.seed(123)\n",
    "m = 1000\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "# Compute jackknife resampling\n",
    "\n",
    "# Standard deviation based\n",
    "mu_s, sigma_mu_s, mu_s_raw = jackknife(data, np.std,\n",
    "                                    kwargs=dict(axis=1, ddof=1),\n",
    "                                    return_raw_distribution=True)\n",
    "\n",
    "pdf1_theory = norm(1, 1. / np.sqrt(2 * (m - 1)))\n",
    "pdf1_jackknife = norm(mu_s, sigma_mu_s)\n",
    "\n",
    "# Sigma_G based\n",
    "mu_sigG, sigma_mu_sigG, mu_sigG_raw = jackknife(data, sigmaG,\n",
    "                                    kwargs=dict(axis=1),\n",
    "                                    return_raw_distribution=True)\n",
    "pdf2_theory = norm(data.std(), 1.06 / np.sqrt(m))\n",
    "pdf2_jackknife = norm(mu_sigG, sigma_mu_sigG)\n",
    "\n",
    "\n",
    "print(f\"mu_s = {mu_s:.3}, sigma_mu_s = {sigma_mu_s:.3}\")\n",
    "print(f\"mu_sigmaG = {mu_sigG:.3}, sigma_mu_sigmaG = {sigma_mu_sigG:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "fig.subplots_adjust(left=0.11, right=0.95, bottom=0.2, top=0.9,\n",
    "                    wspace=0.25)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.hist(mu_s_raw, np.linspace(0.996, 1.008, 100),\n",
    "        label=r'$\\sigma^*\\ {\\rm (std.\\ dev.)}$',\n",
    "        histtype='stepfilled', fc='white', ec='black', density=False)\n",
    "ax.hist(mu_sigG_raw, np.linspace(0.996, 1.008, 100),\n",
    "        label=r'$\\sigma_G^*\\ {\\rm (quartile)}$',\n",
    "        histtype='stepfilled', fc='gray', density=False)\n",
    "ax.legend(loc='upper left', handlelength=2, fontsize = 14)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.004))\n",
    "ax.set_xlabel(r'$\\sigma^*$', fontsize = 14)\n",
    "ax.set_ylabel(r'$N(\\sigma^*)$', fontsize = 14)\n",
    "ax.set_xlim(0.998, 1.008)\n",
    "ax.set_ylim(0, 550)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "x = np.linspace(0.45, 1.15, 1000)\n",
    "ax.plot(x, pdf1_jackknife.pdf(x),\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std.\\ dev.)}$',\n",
    "        zorder=2)\n",
    "ax.plot(x, pdf1_theory.pdf(x), color='gray', zorder=1)\n",
    "\n",
    "ax.plot(x, pdf2_jackknife.pdf(x),\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$', zorder=2)\n",
    "ax.plot(x, pdf2_theory.pdf(x), color='gray', zorder=1)\n",
    "plt.legend(loc='upper left', handlelength=2, fontsize = 14)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$', fontsize = 14)\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$', fontsize = 14)\n",
    "ax.set_xlim(0.45, 1.15)\n",
    "ax.set_ylim(0, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This failure is a general problem with the standard jackknife method, which performs well for smooth differential statistics such as the mean and standard deviation, but does not perform well for medians, quantiles, and other rank-based statistics. For these sorts of statistics, a jackknife implementation that removes more than one observation can overcome this problem. The reason for this failure becomes apparent upon examination of the figure above: for $\\sigma_G$, the vast majority of jackknife samples yield one of three discrete values! Because quartiles are insensitive to the removal of outliers, all samples created by the removal of a point larger than $q_{75}$ lead to precisely the same estimate. The same is true for removal of any point smaller than $q_{25}$, and for any point in the range $q_{25} < x < q_{75}$. Because of this, the jackknife cannot accurately sample the error distribution, which leads to a gross misestimate of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convinient functions in astropy\n",
    "\n",
    "from astropy.stats import jackknife_stats\n",
    "\n",
    "x = np.random.normal(loc=0, scale=1, size=1000)\n",
    "estimate, bias, stderr, conf_interval = jackknife_stats(x, np.std)\n",
    "print(estimate , stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejecting a null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flip a coin eight times and get six tails; should we reject the hypothesis that the coin is fair? We will assume the null hypothesis that the coin is indeed fair. Recall that we can find probabilities of coin flips with the binomial distribution,\n",
    "\n",
    "$$ p(k|b,N) = \\frac{N!}{k!(N-k)!} b^k (1-b)^{N-k}. $$\n",
    "\n",
    "Since p-values are defined as the probability that something *at least* as extreme as your data could have occurred (assuming the null hypothesis is correct), we can find the p-value by adding the probability of 6/8, 7/8, and 8/8 tails.\n",
    "\n",
    "$$ \\frac{8!}{6!2!}\\frac{1}{2}^6 \\frac{1}{2}^2 + \\frac{8!}{7!1!}\\frac{1}{2}^7 \\frac{1}{2}^1 + \\frac{8!}{8!0!}\\frac{1}{2}^8 \\frac{1}{2}^0$$\n",
    "\n",
    "We get that the probability of this occurring is 0.145; thus, we cannot reject the null hypothesis at the 0.05 significance level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the example in class. \n",
    "\n",
    "Assume that $h_B(x) = \\mathcal{N} (\\mu = 100, \\sigma = 10) $ and $h_s(x) = \\mathcal{N} (\\mu = 150, \\sigma = 12)$, with $a$ = 0.1 and $N = 10^6$ (this will be image with 1000 x 1000 resolution elements; the $x$ values correspond to the sum of background and source counts). We will plot these two distributions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Generate and draw the curves\n",
    "x = np.linspace(50, 200, 1000)\n",
    "p1 = 0.9 * norm(100, 10).pdf(x)\n",
    "p2 = 0.1 * norm(150, 12).pdf(x)\n",
    "\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.fill(x, p1, ec='k', fc='#AAAAAA', alpha=0.5)\n",
    "ax.fill(x, p2, '-k', fc='#AAAAAA', alpha=0.5)\n",
    "\n",
    "# plot x_c = 120\n",
    "ax.plot([120, 120], [0.0, 0.04], '--k')\n",
    "\n",
    "ax.text(100, 0.036, r'$h_B(x)$', ha='center', va='bottom', fontsize = 14)\n",
    "ax.text(150, 0.0035, r'$h_S(x)$', ha='center', va='bottom', fontsize = 14)\n",
    "ax.text(122, 0.039, r'$x_c=120$', ha='left', va='top', fontsize = 14)\n",
    "ax.text(125, 0.01, r'$(x > x_c\\ {\\rm classified\\ as\\ sources})$', fontsize = 14)\n",
    "\n",
    "ax.set_xlim(50, 200)\n",
    "ax.set_ylim(0, 0.04)\n",
    "\n",
    "ax.set_xlabel('$x$', fontsize = 14)\n",
    "ax.set_ylabel('$p(x)$', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we naively choose $x_c$=120 (a \"$2\\sigma$ cut” away from the mean for $h_B$, corresponding to a Type I error probability of $\\alpha$ = 0.024 ((1-95.45\\%)/2), **21,600 values will be incorrectly classified as a source!** The sample completeness for this value of $x_c$ is 0.994 and **99,400 values are correctly classified as a source.** Although the Type I error rate is only 0.024, the sample contamination is 21,600/(21,600+99,400) = 0.179, or over 7 times higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of distributions: KS-tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "np.random.seed(4)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.step(np.sort(stats.norm.rvs(0,3,25)), np.linspace(0, 1, 25) ,lw = 3)\n",
    "plt.plot(np.sort(stats.norm.rvs(0,3,1000)), np.linspace(0, 1, 1000), lw=3)\n",
    "\n",
    "plt.annotate(\"\", xy=(2.3, 0.965), xytext=(2.3, 0.77),\n",
    "            arrowprops=dict(arrowstyle=\"<->\",lw=2))\n",
    "\n",
    "plt.text(2.6,0.86, \"D\", fontsize = 20)\n",
    "\n",
    "plt.legend(['CDF 1', 'CDF 2'])\n",
    "plt.title('Comparing CDFs for K-S test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask if a sample is from a certain kind of distribution\n",
    "np.random.seed(0)\n",
    "vals = np.random.normal(loc=0, scale=1, size= 1000)\n",
    "\n",
    "print(f'Normal: {stats.kstest(vals, \"norm\")}')\n",
    "print(f'Uniform: {stats.kstest(vals, \"uniform\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask if two samples come from the same underlying distribution\n",
    "\n",
    "np.random.seed(0)\n",
    "sample1 = np.random.uniform(low=0.0, high=1.0,size=100)\n",
    "sample2 = np.random.normal(loc=0.0, scale=1.0,size=110)\n",
    "sample3 = np.random.normal(loc=0.0, scale=1.0,size=95)\n",
    "\n",
    "print(f'Uniform vs. Normal: {stats.ks_2samp(sample1, sample2)}')\n",
    "print(f'Normal vs. Normal: {stats.ks_2samp(sample2, sample3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of Gaussianity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anderson-Darling test**\n",
    "\n",
    "The test is based on the statistic \n",
    "\n",
    "$$ A^2 = -N - \\frac{1}{N}\\sum^N_{i=1}[(2i-1)\\ln(F_i)+(2N-2i+1)\\ln(1-F_i)] $$\n",
    "\n",
    "where $F_i$ is the $i$th value of the cumulative distribution function $z_i$, which is defined as \n",
    "\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma}$$\n",
    "\n",
    "and assumed to be in ascending order. In this expression, either one or both of $\\mu$ and $\\sigma$ can be known or determined from data $\\{x_i\\}$. Assuming Gaussianity, one can consult a lookup table for $A^2$ to determine whether the hypothesis of Gauussianity of rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shapiro-Wilk test**\n",
    "\n",
    "Based on both data values $x_i$ and data ranks $R_i$:\n",
    "\n",
    "$$W = \\frac{\\big(\\sum^N_{i=1}a_iR_i\\big)^2}{\\sum^N_{i=1}(x_i-\\overline{x})^2} $$\n",
    "\n",
    "where constants $a_i$ encode the expected values of the order statistics for random variables sampled from the standard normal distribution (the test's null hypothesis) -- draw N samples from a standard normal distribution and order them. The Shapiro-Wilk test is very sensitive to non-Gaussian tails of the distribution (\"outliers\") but not as much to detailed departures from Gaussianity in the distribution's core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.stats import mean_sigma, median_sigmaG\n",
    "\n",
    "# create distributions\n",
    "np.random.seed(1)\n",
    "normal_vals = stats.norm(0, 1).rvs(10000) # singular Gaussian\n",
    "\n",
    "dual_vals = stats.norm(0, 1).rvs(10000)\n",
    "dual_vals[:4000] = stats.norm(3,2).rvs(4000) # mixture of two Gaussians\n",
    "\n",
    "x = np.linspace(-4, 10, 1000)\n",
    "normal_pdf = stats.norm(0, 1).pdf(x) # pdf for singular Gaussian\n",
    "dual_pdf = 0.6 * stats.norm(0, 1).pdf(x) + 0.4 * stats.norm(3, 2).pdf(x) \n",
    "#pdf for mixture of two Gaussians\n",
    "\n",
    "vals = [normal_vals, dual_vals]\n",
    "pdf = [normal_pdf, dual_pdf]\n",
    "xlims = [(-4, 4), (-4, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the statistics and plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(2):\n",
    "    ax = fig.add_subplot(2,1,i+1)\n",
    "\n",
    "    # compute statistics\n",
    "    \n",
    "    # Anderson-Darling test\n",
    "    A2, sig, crit = stats.anderson(vals[i]) \n",
    "    \n",
    "    # KS test\n",
    "    D, pD = stats.kstest(vals[i], \"norm\")  \n",
    "    \n",
    "    # Shapiro test\n",
    "    W, pW = stats.shapiro(vals[i])  \n",
    "    \n",
    "    mu, sigma = mean_sigma(vals[i], ddof=1)\n",
    "    median, sigmaG = median_sigmaG(vals[i])\n",
    "\n",
    "\n",
    "    # display results in a table\n",
    "    print(70 * '_')\n",
    "    print(\"  Kolmogorov-Smirnov test: D = %.2g  p = %.2g \" % (D, pD))\n",
    "    print(\"  Anderson-Darling test: A^2 = %.2g\" % A2)\n",
    "    print(\"    significance  | critical value \")\n",
    "    print(\"    --------------|----------------\")\n",
    "    for j in range(len(sig)):\n",
    "        print(\"    {0:.2f}          | {1:.1f}%\".format(sig[j], crit[j]))\n",
    "    print(\"  Shapiro-Wilk test: W = %.2g p = %.2g\" % (W, pW))\n",
    "\n",
    "\n",
    "    # plot a histogram\n",
    "    ax.hist(vals[i], bins=50, density=True, histtype='stepfilled', alpha=0.5)\n",
    "    ax.plot(x, pdf[i], '-k')\n",
    "    ax.set_xlim(xlims[i])\n",
    "\n",
    "    # print information on the plot\n",
    "    info = \"Anderson-Darling: $A^2 = %.2f$\\n\" % A2\n",
    "    info += \"Kolmogorov-Smirnov: $D = %.2g$\\n\" % D\n",
    "    info += \"Shapiro-Wilk: $W = %.2g$\\n\" % W\n",
    "    ax.text(0.97, 0.97, info, ha='right', va='top', \n",
    "            transform=ax.transAxes, fontsize = 12)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylim(0, 0.55)\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "    else:\n",
    "        ax.set_ylim(0, 0.35)\n",
    "        ax.set_xlabel('$x$', fontsize = 14)\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "        \n",
    "    ax.set_ylabel('$p(x)$', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
