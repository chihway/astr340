{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class notebook: 2024-02-21\n",
    "\n",
    "In this notebook, we will look at a few different classification techniques. In the first part, we demonstrate this on a set of SDSS RR lyrae stars. This will be a sample of 92,658 nonvariable stars and 483 RR Lyraes. The classification will be based on their colors (u-g, g-r, r-i, i-z). In the second part we apply decision tree methods to the problem of photometric redshifts.   \n",
    "\n",
    "This notebook is intended to support Chapter 9 of the textbook, and material is taken from the following script (from astroML):\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter9/astroml_chapter9_Classification.ipynb\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter9/fig_rrlyrae_decisiontree.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter9/fig_photoz_tree.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter9/fig_photoz_forest.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter9/fig_photoz_nn.py\n",
    "\n",
    "\n",
    "We might not have the time, but if you want to see an example of deep learning, this is a good starting point: https://github.com/astroML/astroML-notebooks/blob/main/chapter9/astroml_chapter9_Deep_Learning_Classifying_Astronomical_Images.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at RR Lyraes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "# get data and split into training & testing sets\n",
    "# data is compiled via this script: \n",
    "# https://github.com/astroML/astroML/blob/main/astroML/datasets/rrlyrae_mags.py\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns (g-r, u-g, r-i, i-z)\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)        # total number\n",
    "N_st = np.sum(y == 0) # number of non-RR stars\n",
    "N_rr = N_tot - N_st   # number of RR\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayse\n",
    "\n",
    "In Gaussian naive Bayes $p_k(x^i)$ are modeled as one-dimensional normal distributions, with means $\\mu^i_k$ and widths $\\sigma^i_k$. The naive Bayes estimator is then\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{y_k}\\left[\\ln \\pi_k - \\frac{1}{2}\\sum_{i=1}^N\\left(2\\pi(\\sigma^i_k)^2 + \\frac{(x^i - \\mu^i_k)^2}{(\\sigma^i_k)^2} \\right) \\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifiers = []\n",
    "predictions = []\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "order = np.array([1, 0, 2, 3])\n",
    "\n",
    "# loop through training on 1,2,3,4 colors\n",
    "for nc in Ncolors: \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train[:, :nc], y_train)\n",
    "    y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "    classifiers.append(clf)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifiers)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the decision boundary with 2 colors\n",
    "clf = classifiers[1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 81),\n",
    "                     np.linspace(ylim[0], ylim[1], 71))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z[:, 1].reshape(xx.shape)\n",
    "print(np.c_[yy.ravel(), xx.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "# plot the predicted probability\n",
    "im = ax.imshow(Z, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "\n",
    "# plot 50% boundary\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# Plot completeness vs Ncolors\n",
    "ax = plt.subplot(222)\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# Plot contamination vs Ncolors\n",
    "ax = plt.subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear discriminant analysis (LDA) and Quadratic discriminant analysis (QDA)\n",
    "\n",
    "Linear discriminant analysis (LDA) assumes  the class distributions have identical\n",
    "covariances for all $k$ classes (all classes are a set of shifted Gaussians). The\n",
    "optimal classifier is derived from the log of the class\n",
    "posteriors \n",
    "\n",
    "$$g_k(\\vec{x}) = \\vec{x}^T \\Sigma^{-1} \\vec{\\mu_k} - \\frac{1}{2}\\vec{\\mu_k}^T \\Sigma^{-1} \\vec{\\mu_k} + \\log \\pi_k,\n",
    "$$\n",
    "\n",
    "with $\\vec{\\mu_k}$ the mean of class $k$ and $\\Sigma$ the covariance of the\n",
    "Gaussians. The class dependent covariances that would normally give rise to a quadratic dependence on\n",
    "$\\vec{x}$ cancel out if they are assumed to be constant. The Bayes classifier is, therefore, linear with respect to $\\vec{x}$.\n",
    "\n",
    "The discriminant boundary between classes is the line that minimizes\n",
    "the overlap between Gaussians\n",
    "\n",
    "$$  g_k(\\vec{x}) - g_\\ell(\\vec{x}) = \\vec{x}^T \\Sigma^{-1} (\\mu_k-\\mu_\\ell)  - \\frac{1}{2}(\\mu_k - \\mu_\\ell)^T \\Sigma^{-1}(\\mu_k -\\mu_\\ell)  + \\log (\\frac{\\pi_k}{\\pi_\\ell}) = 0. $$\n",
    "\n",
    "\n",
    "\n",
    "Relaxing the requirement that the covariances of the\n",
    "Gaussians are constant, the discriminant function\n",
    "becomes quadratic in $x$:\n",
    "\n",
    "$$ g(\\vec{x}) = -\\frac{1}{2} \\log | \\Sigma_k |   - \\frac{1}{2}(\\vec{x}-\\mu_k)^T C^{-1}(\\vec{x}-\\mu_k) + \\log \\pi_k. $$\n",
    "\n",
    "This is sometimes known as _quadratic discriminant analysis_ (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import (LinearDiscriminantAnalysis,\n",
    "                                           QuadraticDiscriminantAnalysis)\n",
    "# perform LinearDiscriminantAnalysis\n",
    "classifiers = []\n",
    "predictions = []\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "for nc in Ncolors:\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train[:, :nc], y_train)\n",
    "    y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "    classifiers.append(clf)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"LDA\")\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n",
    "\n",
    "# perform QuadraticDiscriminantAnalysis\n",
    "qclassifiers = []\n",
    "qpredictions = []\n",
    "\n",
    "for nc in Ncolors:\n",
    "    qlf = QuadraticDiscriminantAnalysis()\n",
    "    qlf.fit(X_train[:, :nc], y_train)\n",
    "    qy_pred = qlf.predict(X_test[:, :nc])\n",
    "\n",
    "    qclassifiers.append(qlf)\n",
    "    qpredictions.append(qy_pred)\n",
    "\n",
    "qpredictions = np.array(qpredictions)\n",
    "\n",
    "qcompleteness, qcontamination = completeness_contamination(qpredictions, y_test)\n",
    "\n",
    "print(\"QDA\")\n",
    "print(\"completeness\", qcompleteness)\n",
    "print(\"contamination\", qcontamination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the decision boundary\n",
    "clf = classifiers[1]\n",
    "qlf = qclassifiers[1]\n",
    "\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71),\n",
    "                     np.linspace(ylim[0], ylim[1], 81))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z[:, 1].reshape(xx.shape)\n",
    "\n",
    "QZ = qlf.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "QZ = QZ[:, 1].reshape(xx.shape)\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6, label='unweighted')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6, label='unweighted')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "####################\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(QZ, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "\n",
    "ax.contour(xx, yy, QZ, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6, label='unweighted')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6, label='unweighted')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model (GMM)\n",
    "\n",
    "The natural extension to the Gaussian assumptions is to use GMM's to learn the density distribution. \n",
    "\n",
    "The number of Gaussian components $K$ must be chosen for each class independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a wrapper of sklearn.mixture.GaussianMixture\n",
    "# https://github.com/astroML/astroML/blob/main/astroML/classification/gmm_bayes.py\n",
    "from astroML.classification import GMMBayes\n",
    "\n",
    "# GMM-bayes takes several minutes to run, and is order[N^2]\n",
    "#  truncating the dataset can be useful for experimentation.\n",
    "#X_tr = X[::10]\n",
    "#y_tr = y[::10]\n",
    "\n",
    "\n",
    "# perform GMM Bayes\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "Ncomp = [1, 3]\n",
    "\n",
    "\n",
    "def compute_GMMbayes(Ncolors, Ncomp):\n",
    "    classifiers = []\n",
    "    predictions = []\n",
    "\n",
    "    for ncm in Ncomp:\n",
    "        classifiers.append([])\n",
    "        predictions.append([])\n",
    "        for nc in Ncolors:\n",
    "            clf = GMMBayes(ncm, tol=1E-5, covariance_type='full')\n",
    "            clf.fit(X_train[:, :nc], y_train)\n",
    "            y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "            classifiers[-1].append(clf)\n",
    "            predictions[-1].append(y_pred)\n",
    "\n",
    "    return classifiers, predictions\n",
    "\n",
    "classifiers, predictions = compute_GMMbayes(Ncolors, Ncomp)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the decision boundary for using 3 Gaussian components to classify on 2 colors\n",
    "clf = classifiers[1][1] \n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71),\n",
    "                     np.linspace(ylim[0], ylim[1], 81))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z[:, 1].reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness[0], '^--k', ms=6, label='N=%i' % Ncomp[0])\n",
    "ax.plot(Ncolors, completeness[1], 'o-k', ms=6, label='N=%i' % Ncomp[1])\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], '^--k', ms=6, label='N=%i' % Ncomp[0])\n",
    "ax.plot(Ncolors, contamination[1], 'o-k', ms=6, label='N=%i' % Ncomp[1])\n",
    "ax.legend(loc='lower right',\n",
    "          bbox_to_anchor=(1.0, 0.78))\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbours\n",
    "\n",
    "As with density estimation (and kernel density estimation)  the intuitive justification is that $p(y|x) \\approx p(y|x')$ if $x'$ is very close to $x$.  \n",
    "\n",
    "The number of neighbors, $K$, regulates the complexity of the classification. In simplest form, a majority rule classification is adopted, where each of the $K$ points votes on the classification. Increasing $K$ decreases the variance in the classification but at the expense of an increase in the bias.  \n",
    "\n",
    "Weights can be assigned to  individual votes by weighting the vote by the distance to the nearest point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# perform Classification\n",
    "\n",
    "classifiers = []\n",
    "predictions = []\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "# look at 2 values of K\n",
    "kvals = [1, 10]\n",
    "\n",
    "for k in kvals:\n",
    "    classifiers.append([])\n",
    "    predictions.append([])\n",
    "    for nc in Ncolors:\n",
    "        clf = KNeighborsClassifier(n_neighbors=k)\n",
    "        clf.fit(X_train[:, :nc], y_train)\n",
    "        y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "        classifiers[-1].append(clf)\n",
    "        predictions[-1].append(y_pred)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the decision boundary\n",
    "clf = classifiers[1][1] # make plot for k=10, 2 colors, also try to do k=1\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71),\n",
    "                     np.linspace(ylim[0], ylim[1], 81))\n",
    "\n",
    "Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 2)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "ax.text(0.02, 0.02, \"k = %i\" % kvals[1],\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "\n",
    "ax.plot(Ncolors, completeness[0], 'o-k', ms=6, label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, completeness[1], '^--k', ms=6, label='k=%i' % kvals[1])\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], 'o-k', ms=6, label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, contamination[1], '^--k', ms=6, label='k=%i' % kvals[1])\n",
    "ax.legend(loc='lower right',\n",
    "          bbox_to_anchor=(1.0, 0.79))\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "Find the hyperplane that maximizes the distance of the closest point from either class. This distance is the margin (width of the line before it hits a point). We want the line that maximizes the margin (m).\n",
    "\n",
    "The points on the margin are called _support vectors_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM takes several minutes to run, and is order[N^2]\n",
    "#  truncating the dataset can be useful for experimentation.\n",
    "#X_tr = X[::5]\n",
    "#y_tr = y[::5]\n",
    "\n",
    "# Fit Kernel SVM\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "def compute_SVM(Ncolors):\n",
    "    classifiers = []\n",
    "    predictions = []\n",
    "\n",
    "    for nc in Ncolors:\n",
    "        # perform support vector classification\n",
    "        clf = SVC(kernel='rbf', gamma=20.0, class_weight='balanced')\n",
    "        clf.fit(X_train[:, :nc], y_train)\n",
    "        y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "        classifiers.append(clf)\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "    return classifiers, predictions\n",
    "\n",
    "classifiers, predictions = compute_SVM(Ncolors)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the decision boundary\n",
    "clf = classifiers[1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 101),\n",
    "                     np.linspace(ylim[0], ylim[1], 101))\n",
    "\n",
    "Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth the boundary\n",
    "from scipy.ndimage import gaussian_filter\n",
    "Z = gaussian_filter(Z, 2)\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Fit Decision tree\n",
    "classifiers = []\n",
    "predictions = []\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "depths = [7, 12] # set layers of the tree\n",
    "\n",
    "for depth in depths:\n",
    "    classifiers.append([])\n",
    "    predictions.append([])\n",
    "    for nc in Ncolors:\n",
    "        clf = DecisionTreeClassifier(random_state=0, max_depth=depth,\n",
    "                                     criterion='entropy')\n",
    "        clf.fit(X_train[:, :nc], y_train)\n",
    "        y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "        classifiers[-1].append(clf)\n",
    "        predictions[-1].append(y_pred)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the decision boundary for depth 12 and 2 colors\n",
    "clf = classifiers[1][1] \n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 101),\n",
    "                     np.linspace(ylim[0], ylim[1], 101))\n",
    "\n",
    "Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "ax.text(0.02, 0.02, \"depth = %i\" % depths[1],\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness[0], 'o-k', ms=6, label=\"depth=%i\" % depths[0])\n",
    "ax.plot(Ncolors, completeness[1], '^--k', ms=6, label=\"depth=%i\" % depths[1])\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], 'o-k', ms=6, label=\"depth=%i\" % depths[0])\n",
    "ax.plot(Ncolors, contamination[1], '^--k', ms=6, label=\"depth=%i\" % depths[1])\n",
    "ax.legend(loc='lower right',\n",
    "          bbox_to_anchor=(1.0, 0.79))\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next look at photo-z's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "\n",
    "# Fetch data and prepare it for the computation\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 separate points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Compute the cross-validation scores for several tree depths\n",
    "depth = np.arange(1, 21)\n",
    "rms_test = np.zeros(len(depth))\n",
    "rms_train = np.zeros(len(depth))\n",
    "i_best = 0\n",
    "z_fit_best = None\n",
    "\n",
    "for i, d in enumerate(depth):\n",
    "    clf = DecisionTreeRegressor(max_depth=d, random_state=0)\n",
    "    clf.fit(mag_train, z_train)\n",
    "\n",
    "    z_fit_train = clf.predict(mag_train)\n",
    "    z_fit = clf.predict(mag_test)\n",
    "    \n",
    "    # training and test errors\n",
    "    rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "    rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "    if rms_test[i] <= rms_test[i_best]:\n",
    "        i_best = i\n",
    "        z_fit_best = z_fit\n",
    "\n",
    "best_depth = depth[i_best]\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# first panel: cross-validation\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "# second panel: best-fit results\n",
    "ax = fig.add_subplot(122)\n",
    "edges = np.linspace(z_test.min(), z_test.max(), 101)\n",
    "H, zs_bins, zp_bins = np.histogram2d(z_test, z_fit_best, bins=edges)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto', \n",
    "           extent=[zs_bins[0], zs_bins[-1], zs_bins[0], zs_bins[-1]],\n",
    "           cmap=plt.cm.Oranges)\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.04, 0.96, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from astroML.utils.decorators import pickle_results\n",
    "\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_forest.pkl')\n",
    "def compute_photoz_forest(depth):\n",
    "    rms_test = np.zeros(len(depth))\n",
    "    rms_train = np.zeros(len(depth))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, d in enumerate(depth):\n",
    "        clf = RandomForestRegressor(n_estimators=10,\n",
    "                                    max_depth=d, random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.arange(1, 21)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(depth)\n",
    "best_depth = depth[i_best]\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "edges = np.linspace(z_test.min(), z_test.max(), 101)\n",
    "H, zs_bins, zp_bins = np.histogram2d(z_test, z_fit_best, bins=edges)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto', \n",
    "           extent=[zs_bins[0], zs_bins[-1], zs_bins[0], zs_bins[-1]],\n",
    "           cmap=plt.cm.Oranges)\n",
    "\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to pip install torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put magnitudes into array\n",
    "# normalize to zero mean and unit variance for easier training\n",
    "datanormed = np.zeros((len(data), 6), dtype=np.float32)\n",
    "for i, band in enumerate(['u', 'g', 'r', 'i', 'z']):\n",
    "    band = 'modelMag_' + band\n",
    "    datanormed[:, i] = (data[band] - data[band].mean()) / data[band].std()\n",
    "\n",
    "# put redshifts into array\n",
    "datanormed[:, 5] = data['z']\n",
    "\n",
    "# split data into 9:1 train:test\n",
    "dataset = torchdata.TensorDataset(torch.tensor(datanormed[:, 0:5]),\n",
    "                                  torch.tensor(datanormed[:, 5]).view(-1, 1))\n",
    "trainnum = datanormed.shape[0] // 10 * 9\n",
    "traindata, testdata = torchdata.random_split(dataset, \n",
    "                                             [trainnum, datanormed.shape[0] - trainnum])\n",
    "traindataloader = torchdata.DataLoader(traindata, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define structure of neural net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nhidden):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc_h = nn.Linear(5, nhidden) # hidden layer\n",
    "        self.fc_o = nn.Linear(nhidden, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc_h(x)) # activation\n",
    "        z = self.fc_o(h)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pickle_results('NNphotoz.pkl')\n",
    "def train_NN():\n",
    "    model = Net(4)\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=5, threshold=1e-3)\n",
    "\n",
    "    min_valid_loss = float('inf')\n",
    "    badepochs = 0\n",
    "    \n",
    "    # train for 10 epochs, this is quite low, try higher when you have time\n",
    "    # thought it appears to be already pretty good\n",
    "    for t in range(10):\n",
    "        print(t)\n",
    "        train_loss = 0\n",
    "        for i, databatch in enumerate(traindataloader, 0):\n",
    "            photometry, redshifts = databatch\n",
    "            optimizer.zero_grad()\n",
    "            z_pred = model(photometry)\n",
    "            loss = criterion(z_pred, redshifts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            photometry = testdata[:][0]\n",
    "            redshifts = testdata[:][1]\n",
    "            z_pred = model(photometry)\n",
    "            valid_loss = criterion(z_pred, redshifts)\n",
    "            if t % 10 == 0:\n",
    "                print('Epoch %3i: train loss %0.3e validation loss %0.3e' % (t, \\\n",
    "                        train_loss / len(traindata), valid_loss / len(testdata)))\n",
    "            # stop training if validation loss has not fallen in 10 epochs\n",
    "            if valid_loss > min_valid_loss*(1-1e-3):\n",
    "                badepochs += 1\n",
    "            else:\n",
    "                min_valid_loss = valid_loss\n",
    "                badepochs = 0\n",
    "            if badepochs == 10:\n",
    "                print('Finished training')\n",
    "                break\n",
    "        scheduler.step(valid_loss)\n",
    "    return model\n",
    "\n",
    "model = train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    photometry = testdata[:][0]\n",
    "    redshifts = testdata[:][1]\n",
    "    z_pred = model(photometry)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    fig.subplots_adjust(wspace=0.25,\n",
    "                        left=0.1, right=0.95,\n",
    "                        bottom=0.15, top=0.9)\n",
    "\n",
    "    ax = plt.axes()\n",
    "    #ax.scatter(redshifts, z_pred, s=1, lw=0, c='k')\n",
    "    H, zs_bins, zp_bins = np.histogram2d(redshifts.numpy().flatten(), z_pred.numpy().flatten(), 151)\n",
    "    ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "               extent=[zs_bins[0], zs_bins[-1], zp_bins[0], zp_bins[-1]],\n",
    "               cmap=plt.cm.Oranges)\n",
    "    ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "    rms = np.sqrt(np.mean((z_pred-redshifts).numpy()**2))\n",
    "    ax.text(0, 0.35, 'RMS error = %0.3f' % np.sqrt(np.mean((z_pred-redshifts).numpy()**2)))\n",
    "    ax.set_xlim(-0.02, 0.4001)\n",
    "    ax.set_ylim(-0.02, 0.4001)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "    ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "    ax.set_ylabel(r'$z_{\\rm fit}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
