{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class notebook: 2024-01-22\n",
    "\n",
    "In this notebook, we will look at some common usages of classical statistical inference. We first look at empirical estimates of error bars, then look at hypothesis testing and ways to compare distributions. \n",
    "\n",
    "This notebook is intended to support Chapter 4.6-4.9 of the textbook, and material is taken from the following scripts (from astroML):\n",
    "\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Hypothesis_testing.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Comparison_of_distributions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejecting a null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flip a coin eight times and get six tails; should we reject the hypothesis that the coin is fair? We will assume the null hypothesis that the coin is indeed fair. Recall that we can find probabilities of coin flips with the binomial distribution,\n",
    "\n",
    "$$ p(k|b,N) = \\frac{N!}{k!(N-k)!} b^k (1-b)^{N-k}. $$\n",
    "\n",
    "Since p-values are defined as the probability that something *at least* as extreme as your data could have occurred (assuming the null hypothesis is correct), we can find the p-value by adding the probability of 6/8, 7/8, and 8/8 tails.\n",
    "\n",
    "$$ \\frac{8!}{6!2!}\\frac{1}{2}^6 \\frac{1}{2}^2 + \\frac{8!}{7!1!}\\frac{1}{2}^7 \\frac{1}{2}^1 + \\frac{8!}{8!0!}\\frac{1}{2}^8 \\frac{1}{2}^0$$\n",
    "\n",
    "We get that the probability of this occurring is 0.145; thus, we cannot reject the null hypothesis at the 0.05 significance level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the example in class. \n",
    "\n",
    "Assume that $h_B(x) = \\mathcal{N} (\\mu = 100, \\sigma = 10) $ and $h_s(x) = \\mathcal{N} (\\mu = 150, \\sigma = 12)$, with $a$ = 0.1 and $N = 10^6$ (this will be image with 1000 x 1000 resolution elements; the $x$ values correspond to the sum of background and source counts). We will plot these two distributions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and draw the curves\n",
    "x = np.linspace(50, 200, 1000)\n",
    "p1 = 0.9 * norm(100, 10).pdf(x)\n",
    "p2 = 0.1 * norm(150, 12).pdf(x)\n",
    "\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.fill(x, p1, ec='k', fc='#AAAAAA', alpha=0.5)\n",
    "ax.fill(x, p2, '-k', fc='#AAAAAA', alpha=0.5)\n",
    "\n",
    "# plot x_c = 120\n",
    "ax.plot([120, 120], [0.0, 0.04], '--k')\n",
    "\n",
    "ax.text(100, 0.036, r'$h_B(x)$', ha='center', va='bottom', fontsize = 14)\n",
    "ax.text(150, 0.0035, r'$h_S(x)$', ha='center', va='bottom', fontsize = 14)\n",
    "ax.text(122, 0.039, r'$x_c=120$', ha='left', va='top', fontsize = 14)\n",
    "ax.text(125, 0.01, r'$(x > x_c\\ {\\rm classified\\ as\\ sources})$', fontsize = 14)\n",
    "\n",
    "ax.set_xlim(50, 200)\n",
    "ax.set_ylim(0, 0.04)\n",
    "\n",
    "ax.set_xlabel('$x$', fontsize = 14)\n",
    "ax.set_ylabel('$p(x)$', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we naively choose $x_c$=120 (a \"$2\\sigma$ cut‚Äù away from the mean for $h_B$, corresponding to a Type I error probability of $\\alpha$ = 0.024 ((1-95.45\\%)/2), **21,600 values will be incorrectly classified as a source!** The sample completeness for this value of $x_c$ is 0.994 and **99,400 values are correctly classified as a source.** Although the Type I error rate is only 0.024, the sample contamination is 21,600/(21,600+99,400) = 0.179, or over 7 times higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of distributions: KS-tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.step(np.sort(stats.norm.rvs(0,3,25)), np.linspace(0, 1, 25) ,lw = 3)\n",
    "plt.plot(np.sort(stats.norm.rvs(0,3,1000)), np.linspace(0, 1, 1000), lw=3)\n",
    "\n",
    "plt.annotate(\"\", xy=(2.3, 0.965), xytext=(2.3, 0.77),\n",
    "            arrowprops=dict(arrowstyle=\"<->\",lw=2))\n",
    "\n",
    "plt.text(2.6,0.86, \"D\", fontsize = 20)\n",
    "\n",
    "plt.legend(['CDF 1', 'CDF 2'])\n",
    "plt.title('Comparing CDFs for K-S test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "vals = np.random.normal(loc=0, scale=1, size= 1000)\n",
    "\n",
    "print(f'Normal: {stats.kstest(vals, \"norm\")}')\n",
    "print(f'Uniform: {stats.kstest(vals, \"uniform\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sample1 = np.random.uniform(low=0.0, high=1.0,size=100)\n",
    "sample2 = np.random.normal(loc=0.0, scale=1.0,size=110)\n",
    "sample3 = np.random.normal(loc=0.0, scale=1.0,size=95)\n",
    "\n",
    "print(f'Uniform vs. Normal: {stats.ks_2samp(sample1, sample2)}')\n",
    "print(f'Normal vs. Normal: {stats.ks_2samp(sample2, sample3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll show an example using `scipy.stats.kstwo`, which performs the two-sided test statistic distribution. Similarly to other `scipy.stats` classes, we can calculate the first four moments using `kstwo.stats`. Additionally, we can compare the histogram of random samples generated using `kstwo.rvs` to the pdf using `kstwo.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstwo\n",
    "\n",
    "# Calculate the first four moments for a given n\n",
    "n = 500\n",
    "mean, var, skew, kurt = kstwo.stats(n, moments='mvsk')\n",
    "\n",
    "#Generate random values\n",
    "r = kstwo.rvs(n, size=1000)\n",
    "\n",
    "#Plot the ksone pdf and histogram\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "x = np.linspace(kstwo.ppf(0.01, n), kstwo.ppf(0.99, n), 100)\n",
    "plt.hist(r, density=True, bins='auto', histtype='stepfilled', alpha = 0.5, label = 'kstwo hist')\n",
    "plt.plot(x, kstwo.pdf(x, n),label='kstwo pdf')\n",
    "plt.xlim([x[0], x[-1]])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check out many of the other tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
