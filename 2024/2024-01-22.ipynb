{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class notebook: 2024-01-22\n",
    "\n",
    "In this notebook, we will look at some common usages of classical statistical inference. We will look at hypothesis testing and ways to compare distributions. \n",
    "\n",
    "This notebook is intended to support Chapter 4.6-4.9 of the textbook, and material is taken from the following scripts (from astroML):\n",
    "\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Hypothesis_testing.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Comparison_of_distributions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejecting a null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flip a coin eight times and get six tails; should we reject the hypothesis that the coin is fair? We will assume the null hypothesis that the coin is indeed fair. Recall that we can find probabilities of coin flips with the binomial distribution,\n",
    "\n",
    "$$ p(k|b,N) = \\frac{N!}{k!(N-k)!} b^k (1-b)^{N-k}. $$\n",
    "\n",
    "Since p-values are defined as the probability that something *at least* as extreme as your data could have occurred (assuming the null hypothesis is correct), we can find the p-value by adding the probability of 6/8, 7/8, and 8/8 tails.\n",
    "\n",
    "$$ \\frac{8!}{6!2!}\\frac{1}{2}^6 \\frac{1}{2}^2 + \\frac{8!}{7!1!}\\frac{1}{2}^7 \\frac{1}{2}^1 + \\frac{8!}{8!0!}\\frac{1}{2}^8 \\frac{1}{2}^0$$\n",
    "\n",
    "We get that the probability of this occurring is 0.145; thus, we cannot reject the null hypothesis at the 0.05 significance level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the example in class. \n",
    "\n",
    "Assume that $h_B(x) = \\mathcal{N} (\\mu = 100, \\sigma = 10) $ and $h_s(x) = \\mathcal{N} (\\mu = 150, \\sigma = 12)$, with $a$ = 0.1 and $N = 10^6$ (this will be image with 1000 x 1000 resolution elements; the $x$ values correspond to the sum of background and source counts). We will plot these two distributions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Generate and draw the curves\n",
    "x = np.linspace(50, 200, 1000)\n",
    "p1 = 0.9 * norm(100, 10).pdf(x)\n",
    "p2 = 0.1 * norm(150, 12).pdf(x)\n",
    "\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.fill(x, p1, ec='k', fc='#AAAAAA', alpha=0.5)\n",
    "ax.fill(x, p2, '-k', fc='#AAAAAA', alpha=0.5)\n",
    "\n",
    "# plot x_c = 120\n",
    "ax.plot([120, 120], [0.0, 0.04], '--k')\n",
    "\n",
    "ax.text(100, 0.036, r'$h_B(x)$', ha='center', va='bottom', fontsize = 14)\n",
    "ax.text(150, 0.0035, r'$h_S(x)$', ha='center', va='bottom', fontsize = 14)\n",
    "ax.text(122, 0.039, r'$x_c=120$', ha='left', va='top', fontsize = 14)\n",
    "ax.text(125, 0.01, r'$(x > x_c\\ {\\rm classified\\ as\\ sources})$', fontsize = 14)\n",
    "\n",
    "ax.set_xlim(50, 200)\n",
    "ax.set_ylim(0, 0.04)\n",
    "\n",
    "ax.set_xlabel('$x$', fontsize = 14)\n",
    "ax.set_ylabel('$p(x)$', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we naively choose $x_c$=120 (a \"$2\\sigma$ cut‚Äù away from the mean for $h_B$, corresponding to a Type I error probability of $\\alpha$ = 0.024 ((1-95.45\\%)/2), **21,600 values will be incorrectly classified as a source!** The sample completeness for this value of $x_c$ is 0.994 and **99,400 values are correctly classified as a source.** Although the Type I error rate is only 0.024, the sample contamination is 21,600/(21,600+99,400) = 0.179, or over 7 times higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of distributions: KS-tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "np.random.seed(4)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.step(np.sort(stats.norm.rvs(0,3,25)), np.linspace(0, 1, 25) ,lw = 3)\n",
    "plt.plot(np.sort(stats.norm.rvs(0,3,1000)), np.linspace(0, 1, 1000), lw=3)\n",
    "\n",
    "plt.annotate(\"\", xy=(2.3, 0.965), xytext=(2.3, 0.77),\n",
    "            arrowprops=dict(arrowstyle=\"<->\",lw=2))\n",
    "\n",
    "plt.text(2.6,0.86, \"D\", fontsize = 20)\n",
    "\n",
    "plt.legend(['CDF 1', 'CDF 2'])\n",
    "plt.title('Comparing CDFs for K-S test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask if a sample is from a certain kind of distribution\n",
    "np.random.seed(0)\n",
    "vals = np.random.normal(loc=0, scale=1, size= 1000)\n",
    "\n",
    "print(f'Normal: {stats.kstest(vals, \"norm\")}')\n",
    "print(f'Uniform: {stats.kstest(vals, \"uniform\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask if two samples come from the same underlying distribution\n",
    "\n",
    "np.random.seed(0)\n",
    "sample1 = np.random.uniform(low=0.0, high=1.0,size=100)\n",
    "sample2 = np.random.normal(loc=0.0, scale=1.0,size=110)\n",
    "sample3 = np.random.normal(loc=0.0, scale=1.0,size=95)\n",
    "\n",
    "print(f'Uniform vs. Normal: {stats.ks_2samp(sample1, sample2)}')\n",
    "print(f'Normal vs. Normal: {stats.ks_2samp(sample2, sample3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of Gaussianity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anderson-Darling test**\n",
    "\n",
    "The test is based on the statistic \n",
    "\n",
    "$$ A^2 = -N - \\frac{1}{N}\\sum^N_{i=1}[(2i-1)\\ln(F_i)+(2N-2i+1)\\ln(1-F_i)] $$\n",
    "\n",
    "where $F_i$ is the $i$th value of the cumulative distribution function $z_i$, which is defined as \n",
    "\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma}$$\n",
    "\n",
    "and assumed to be in ascending order. In this expression, either one or both of $\\mu$ and $\\sigma$ can be known or determined from data $\\{x_i\\}$. Assuming Gaussianity, one can consult a lookup table for $A^2$ to determine whether the hypothesis of Gauussianity of rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shapiro-Wilk test**\n",
    "\n",
    "Based on both data values $x_i$ and data ranks $R_i$:\n",
    "\n",
    "$$W = \\frac{\\big(\\sum^N_{i=1}a_iR_i\\big)^2}{\\sum^N_{i=1}(x_i-\\overline{x})^2} $$\n",
    "\n",
    "where constants $a_i$ encode the expected values of the order statistics for random variables sampled from the standard normal distribution (the test's null hypothesis) -- draw N samples from a standard normal distribution and order them. The Shapiro-Wilk test is very sensitive to non-Gaussian tails of the distribution (\"outliers\") but not as much to detailed departures from Gaussianity in the distribution's core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.stats import mean_sigma, median_sigmaG\n",
    "\n",
    "# create distributions\n",
    "np.random.seed(1)\n",
    "normal_vals = stats.norm(0, 1).rvs(10000) # singular Gaussian\n",
    "\n",
    "dual_vals = stats.norm(0, 1).rvs(10000)\n",
    "dual_vals[:4000] = stats.norm(3,2).rvs(4000) # mixture of two Gaussians\n",
    "\n",
    "x = np.linspace(-4, 10, 1000)\n",
    "normal_pdf = stats.norm(0, 1).pdf(x) # pdf for singular Gaussian\n",
    "dual_pdf = 0.6 * stats.norm(0, 1).pdf(x) + 0.4 * stats.norm(3, 2).pdf(x) \n",
    "#pdf for mixture of two Gaussians\n",
    "\n",
    "vals = [normal_vals, dual_vals]\n",
    "pdf = [normal_pdf, dual_pdf]\n",
    "xlims = [(-4, 4), (-4, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the statistics and plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(2):\n",
    "    ax = fig.add_subplot(2,1,i+1)\n",
    "\n",
    "    # compute statistics\n",
    "    \n",
    "    # Anderson-Darling test\n",
    "    A2, sig, crit = stats.anderson(vals[i]) \n",
    "    \n",
    "    # KS test\n",
    "    D, pD = stats.kstest(vals[i], \"norm\")  \n",
    "    \n",
    "    # Shapiro test\n",
    "    W, pW = stats.shapiro(vals[i])  \n",
    "    \n",
    "    mu, sigma = mean_sigma(vals[i], ddof=1)\n",
    "    median, sigmaG = median_sigmaG(vals[i])\n",
    "\n",
    "\n",
    "    # display results in a table\n",
    "    print(70 * '_')\n",
    "    print(\"  Kolmogorov-Smirnov test: D = %.2g  p = %.2g \" % (D, pD))\n",
    "    print(\"  Anderson-Darling test: A^2 = %.2g\" % A2)\n",
    "    print(\"    significance  | critical value \")\n",
    "    print(\"    --------------|----------------\")\n",
    "    for j in range(len(sig)):\n",
    "        print(\"    {0:.2f}          | {1:.1f}%\".format(sig[j], crit[j]))\n",
    "    print(\"  Shapiro-Wilk test: W = %.2g p = %.2g\" % (W, pW))\n",
    "\n",
    "\n",
    "    # plot a histogram\n",
    "    ax.hist(vals[i], bins=50, density=True, histtype='stepfilled', alpha=0.5)\n",
    "    ax.plot(x, pdf[i], '-k')\n",
    "    ax.set_xlim(xlims[i])\n",
    "\n",
    "    # print information on the plot\n",
    "    info = \"Anderson-Darling: $A^2 = %.2f$\\n\" % A2\n",
    "    info += \"Kolmogorov-Smirnov: $D = %.2g$\\n\" % D\n",
    "    info += \"Shapiro-Wilk: $W = %.2g$\\n\" % W\n",
    "    ax.text(0.97, 0.97, info, ha='right', va='top', \n",
    "            transform=ax.transAxes, fontsize = 12)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylim(0, 0.55)\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "    else:\n",
    "        ax.set_ylim(0, 0.35)\n",
    "        ax.set_xlabel('$x$', fontsize = 14)\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "        \n",
    "    ax.set_ylabel('$p(x)$', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
