{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class notebook: 2024-01-22\n",
    "\n",
    "In this notebook, we will look at some common usages of classical statistical inference. We first look at the maximum likelihood estimator, then look at different ways we empirically estimate error bars (or confidence levels). \n",
    "\n",
    "This notebook is intended to support Chapter 4.1-4.5 of the textbook, and material is taken from the following scripts (from astroML):\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Maximum_Likelihood_Estimation_and_Goodness_of_fit.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Gaussian_mixtures.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Confidence_estimates.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator (MLE) and Goodness-of-fit\n",
    "\n",
    "Let's walk through an example. \n",
    "\n",
    "We have 120 measurements for the apparent magnitude of our star drawn from a Gaussian distribution with $\\mu = 8$ (mean apparent magnitude) and $\\sigma = 2$ (the error in our measurements). In the figure on the left, we will plot the histogram of our measurements along with the true parent distribution plotted over top of it. In the figure on the right, we will calculate $L$, varying the value of $\\mu$ from 0 to 16.\n",
    "\n",
    "$$ \\qquad\\qquad L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\bigg(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\bigg)\\qquad\\qquad (1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "Nsamples= 120\n",
    "mu_true = 8\n",
    "x = np.linspace(0, 16, 1000)\n",
    "\n",
    "# draw 120 pints from a Gaussian with mu = 8 and sigma = 2\n",
    "measurements = np.random.normal(mu_true, 2, Nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate L for 1000 possible values of mu from 0 to 16\n",
    "products = []\n",
    "for i in x:\n",
    "    j = np.prod(norm(measurements, 2).pdf(i))\n",
    "    products.append(j)\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(1,2)                \n",
    "fig.set_size_inches(14,6)   \n",
    "\n",
    "ax[0].hist(measurements, bins='fd', density = True, edgecolor='black');\n",
    "ax[0].plot(x, norm(8, 2).pdf(x));\n",
    "ax[0].set_xlabel('$x$',fontsize = 14)\n",
    "ax[0].set_ylabel('frequency',fontsize = 14)\n",
    "ax[0].set_xlim(0, 16)\n",
    "ax[0].set_xticks(np.arange(0,16,2))\n",
    "\n",
    "ax[1].plot(x,products)\n",
    "ax[1].set_xlim(0, 16)\n",
    "ax[1].set_ylim(0, 8*10**(-107))\n",
    "ax[1].set_xticks(np.arange(0,16, 2))\n",
    "ax[1].set_xlabel('$\\mu$', fontsize = 14)\n",
    "ax[1].set_ylabel(r'$L(\\mu)$',fontsize = 14);\n",
    "\n",
    "\n",
    "print(f\"\"\"Mean of our dataset: {np.mean(measurements):.2}\n",
    "The value of mu that maximizes L: {x[products.index(max(products))]:.2}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, the value of $\\mu$, which maximizes $L$, is the average of our measurements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLE in the case of truncated and censored data\n",
    "\n",
    "The probability of drawing a measurement $x$ is quantified using the selection probability, or selection function, $S(x)$. When $S(x)=0$ for $x > x_{\\text{max}}$ (analogously for $x < x_\\text{min}$), the data set is **truncated** and we know nothing about sources with $x > _{\\text{max}}$ (not even whether they exist or not). A related but different concept is censored data sets, where a measurement of an existing source was attempted, but the value is outside of some known interval (a familiar astronomical case is an “upper limit” for flux measurement when we look for, e.g., an X-ray source in an optical image of the same region on the sky but do not find it).\n",
    "\n",
    "Looking back at our Gaussian example, we will now show how to account for data truncation using the MLE approach. For simplicity, we will assume that the selection function $S(x)=1$ for $x_\\text{min} \\leq x \\leq x_{\\text{max}} $ and $S(x)=0$ otherwise.\n",
    "\n",
    "\n",
    "When dealing with truncated data, the likelihood for a single datum must be a properly normalized pdf; this is accounted for with a renormalization constant. In the case of a Gaussian error distribution (we assume that $\\sigma$ is known), the likelihood for a single data point is\n",
    "\n",
    "$$ p(x_i|\\mu,\\sigma,x_{\\text{min}},x_{\\text{max}}) = C(\\mu,\\sigma, x_{\\text{min}},x_{\\text{max}}) \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\bigg(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\bigg)$$\n",
    "\n",
    "where the renormalization constant is evaluated as\n",
    "\n",
    "$$ C(\\mu,\\sigma,x_{\\text{min}},x_{\\text{max}}) = (P(x_{\\text{max}}|\\mu, \\sigma) - P(x_{\\text{min}}|\\mu,\\sigma))^{-1} $$\n",
    "\n",
    "with the cumulative distribution function (cdf) for a Gaussian centered at $\\mu$ with a standard deviation $\\sigma$ (see equation 3.47 in the textbook).\n",
    "\n",
    "The log-likelihood is\n",
    "\n",
    "$$ \\quad \\ln L(\\mu) = \\text{constant} - \\sum^N_{i=1} \\frac{(x_i-\\mu)^2}{2\\sigma^2} + N \\ln [C(\\mu,\\sigma, x_{\\text{min}},x_{\\text{max}})].\\quad (3) $$\n",
    "\n",
    "\n",
    "This is very close to the equation we had before; the difference is in our third term, which accounts for truncation. Note that because the cdf contains the Gauss error function, there isn't a simple close-form expression for $\\mu^0$.\n",
    "\n",
    "\n",
    "\n",
    "***EXAMPLE***\n",
    ">On the planet of Caladan, meteorites frequently hit the surface. Suppose some collectors are creating a museum of large meteorites, which they've defined as any meteorite larger than 45 meters in diameter. Before they get entered into the museum, they are held in a back room so a new exhibit can be set up. We also know that on this planet, meteorites are normally distributed, centered at 25 meters, with a standard deviation of 13. Suppose you were to enter the back room and find a meteorite of diameter $S_d$. What is your best estimate for the mean diameter of the meteorites that land on Caladan if you knew that the standard deviation is 13 but didn't know the mean was 25?\n",
    "\n",
    "We can answer this by finding the maximum of $\\ln{L}$ given by equation $(3)$, evaluated for $x_\\text{min}$ = 45, $x_\\text{max} = \\infty $, $N$ = 1, $x_1=S_d$, and $\\sigma$ = 13. Whether $\\mu^0$, the value of $\\mu$ which maximizes $L$, is larger or smaller than 25 depends on the exact value of $S_d$. \n",
    "\n",
    "First we will show that the mean value of a $N$(25,13) Gaussian truncated at $x_\\text{min}$ = 45 is ~50.75. For this, we will use `scipy.stats.truncnorm` ,which allows us to define the bounds of our truncated Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import truncnorm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.subplots(figsize=(12, 7.5))\n",
    "\n",
    "x = np.linspace(44, 75, 1000)\n",
    "mean = 25\n",
    "std = 13\n",
    "x_min = 45\n",
    "x_max = np.inf\n",
    "a, b = (x_min - mean) / std, (x_max - mean) / std\n",
    "r = truncnorm.rvs(a, b, loc = mean, scale = std, size=1000)\n",
    "plt.plot(x, truncnorm.pdf(x,a,b,loc = mean, scale = std));\n",
    "plt.hist(r, bins = 'fd', density = True);\n",
    "print(f'Mean = {np.mean(r):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "const = (-1/2)*(np.log(2*np.pi*(13**2)))\n",
    "\n",
    "def neg_log_likelihood(mu, x_1, x_2=0, n=1, sum_term_x2 = 0):\n",
    "    sum_term_x1 = ((x_1 - mu)**2)/(2*(13**2))\n",
    "    \n",
    "    if x_2 != 0:\n",
    "        sum_term_x2 = ((x_2 - mu)**2)/(2*(13**2))\n",
    "    \n",
    "    norm_constant = np.log((norm(mu,13).cdf(x_max) - (norm(mu,13).cdf(x_min)))**(-1))\n",
    "    logL = -(const - (sum_term_x1+sum_term_x2)+(n*norm_constant))\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will minimize it (i.e., maximizing the *positive* log-likelihood) with `scipy.optimize.minimize`, which takes an initial guess for the value of $\\mu$ and our value(s) of $S_d$. We'll see that if $S_d$ is smaller than ~50.75, the implied mean diameter of the meteorites that land on the surface is less than 25 meters. Conversely, if $S_d$ is greater than ~50.75, then $\\mu^0$ is greater than 25. As an example, we will show the MLE given $S_d$ = 49 and $S_d$ = 52 with initial guesses for $\\mu^0$ equal to 15 and 52 respectively. We can also look at the case given two meteorites $S_{d1}$ and $S_{d2}$ just to illustrate the point further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Value of μ which maximizes lnL for:\n",
    "\n",
    "S_d = 49: {minimize(neg_log_likelihood,15,args = (49)).get(\"x\")[0]:.4}\n",
    "S_d = 52: {minimize(neg_log_likelihood,30,args = (52)).get(\"x\")[0]:.4}\n",
    "\n",
    "Values of μ which maximize] lnL for:\n",
    "\n",
    "S_d1 = 49, S_d2 = 50 (both meteorites with diameters below the mean): {minimize(neg_log_likelihood,15,args = (49,50,2)).get(\"x\")[0]:.4}\n",
    "S_d1 = 50, S_d2 = 51 (one above the mean, one below the mean): {minimize(neg_log_likelihood,25,args = (50,51,2)).get(\"x\")[0]:.4}\n",
    "S_d1 = 52, S_d2 = 53 (both meteorites with diameters above the mean): {minimize(neg_log_likelihood,30,args = (52,53,2)).get(\"x\")[0]:.4}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an arbitrary number of meteorites, their mean diameter must be greater than 50.75 to obtain $\\mu^0 > 25$. If all the meteorites in the back had a diameter of around 45 meters, bunched next to the selection threshold, it is likely that the mean diameter of meteorites that land is below 25 meters! Therefore, if you run into a large meteorite, do not automatically assume that all meteorites have large diameters on average because it could be due to selection effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goodness of fit and model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, chi2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "N_samples = 10\n",
    "mu = 25\n",
    "sigma = 13\n",
    "\n",
    "## Correct model\n",
    "x = np.random.normal(mu, sigma, N_samples)\n",
    "mu = np.average(x)\n",
    "chi_2 = np.sum(np.square((x - mu) / sigma))\n",
    "\n",
    "## Incorrect model\n",
    "x_poisson = np.random.poisson(mu, Nsamples)\n",
    "mu_poisson = np.average(x_poisson)\n",
    "chi_2_poisson = np.sum(np.square((x_poisson - mu_poisson) / sigma))\n",
    "\n",
    "#Plot the results\n",
    "fig = plt.subplots(figsize=(12, 7.5))\n",
    "z = np.linspace(0,32,1000)\n",
    "\n",
    "plt.plot(z, chi2.pdf(z, N_samples - 1))\n",
    "plt.axvline(chi_2, color='green',label = f\"Correct model = {chi_2:.3}\");\n",
    "plt.axvline(chi_2_poisson, color='red',label = f\"Incorrect model ={chi_2_poisson:.3}\");\n",
    "\n",
    "plt.ylim(0,0.12)\n",
    "plt.legend(loc = \"upper right\", fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the $\\chi^2$ value drawn from a Poisson distribution is less likely to occur, whereas the $\\chi^2$ value drawn from the Gaussian is more likely.\n",
    "\n",
    "***EXAMPLE*** \n",
    "> Consider the simple case of the luminosity of a single star being measured multiple times. Our model is that of a star with **no intrinsic luminosity variation**.\n",
    "\n",
    "We will examine four different scenarios:\n",
    "- Correct model with correct errors\n",
    "- Correct model with overestimated errors\n",
    "- Correct model with underestimated errors\n",
    "- Incorrect model with correct errors\n",
    "\n",
    "First, we will define $N$, $\\ell^0$ (the constant luminosity of our star), and $\\sigma_\\ell$ (the measurement error).\n",
    "\n",
    "our models, and our errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 50\n",
    "luminosity = 10\n",
    "sigma_L = 0.2\n",
    "\n",
    "t = np.linspace(0, 1, N)\n",
    "L_obs = np.random.normal(luminosity, sigma_L, N)\n",
    "\n",
    "y_vals = [L_obs, L_obs, L_obs, L_obs + 0.5 - t ** 2]\n",
    "y_errs = [sigma_L, sigma_L * 2, sigma_L / 2, sigma_L]\n",
    "titles = ['correct errors',\n",
    "          'overestimated errors',\n",
    "          'underestimated errors',\n",
    "          'incorrect model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2, 2, 1 + i, xticks=[])\n",
    "\n",
    "    # compute the mean and the chi^2/dof\n",
    "    mu = np.mean(y_vals[i])\n",
    "    z = (y_vals[i] - mu) / y_errs[i]\n",
    "    chi2 = np.sum(z ** 2)\n",
    "    chi2dof = chi2 / (N - 1)\n",
    "\n",
    "    # compute the standard deviations of chi^2/dof\n",
    "    sigma = np.sqrt(2. / (N - 1))\n",
    "    nsig = (chi2dof - 1) / sigma\n",
    "\n",
    "    # plot the points with errorbars\n",
    "    ax.errorbar(t, y_vals[i], y_errs[i],fmt='.k', ecolor='gray', lw=1,ms = 12)\n",
    "    ax.plot([-0.1, 1.3], [luminosity, luminosity], ':k', lw=3)\n",
    "\n",
    "    # Add labels and text\n",
    "    ax.text(0.95, 0.95, titles[i], ha='right', va='top',\n",
    "            transform=ax.transAxes,fontsize = 15,\n",
    "            bbox=dict(boxstyle='round', fc='w', ec='k'))\n",
    "    ax.text(0.02, 0.02, r'$\\hat{\\mu} = %.2f$' % mu, ha='left', va='bottom',\n",
    "            transform=ax.transAxes, fontsize = 15)\n",
    "    ax.text(0.98, 0.02, r'$\\chi^2_{\\rm dof} = %.2f\\, (%.2g\\,\\sigma)$'\n",
    "            % (chi2dof, nsig), ha='right', va='bottom', \n",
    "            transform=ax.transAxes,fontsize = 15)\n",
    "\n",
    "    # set axis limits\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_ylim(8.6, 11.4)\n",
    "\n",
    "    # set ticks and labels\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    if i > 1: ax.set_xlabel('observations', fontsize = 18)\n",
    "    if i % 2 == 0: ax.set_ylabel('Luminosity',fontsize = 18)\n",
    "    else: ax.yaxis.set_major_formatter(plt.NullFormatter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\chi^2_{\\text{dof}} \\approx 1$ indicates that the model fits the data well (upper-left panel). $\\chi^2_{\\text{dof}}$ much smaller than 1 (upper-right panel) is an indication that the errors are overestimated. $\\chi^2_{\\text{dof}}$ much larger than 1 is an indication either that the errors are underestimated (lower-left panel) or that the model is not a good description of the data (lower-right panel). In this last case, it is clear from the data that the star’s luminosity is varying with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE with Gaussian Mixture Model\n",
    "\n",
    "\n",
    "Imagine we have a Gaussian mixture of distributions $\\mathcal{N}(-1,1.5)$, $\\mathcal{N}(0,1)$, and $\\mathcal{N}(3,0.5)$. In this case, $M$ = 3 (number of separate Gaussian distributions), $\\boldsymbol{\\theta}$ includes the normalization factors for each distribution, $\\alpha_1$,$\\alpha_2$,and $\\alpha_3$ as well as the descriptive parameters $\\mu_1$,$\\sigma_1$,$\\mu_2$,$\\sigma_2$,and $\\mu_3$,$\\sigma_3$\n",
    "\n",
    "\n",
    "First we will define our distributions and combine them using `numpy.concatenate`. Then we will create models using `sklearn.mixture.GaussianMixture`that range from one class to ten classes and calculate the AIC and BIC to find the optimal number of classes for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "random_state = np.random.RandomState(seed=1)\n",
    "\n",
    "X = np.concatenate([random_state.normal(-1, 1.5, 350),\n",
    "                    random_state.normal(0, 1, 500),\n",
    "                    random_state.normal(3, 0.5, 150)]).reshape(-1, 1)\n",
    "\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot our results. By using `np.argmin` on our AIC and BIC arrays, we can find the model with the most optimal  $M$ value. After this, we will use `.score_samples` on this model to compute the log-likelihood (the PDF of the sum of Gaussians). Then we can use `.predict_proba` on our log-likelihood to get the density of the $j$th component.\n",
    "\n",
    "Afterward, we can plot `pdf`, our Gaussian mixture, and `pdf_individual` for the three separate Gaussians along with the histogram of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "x = np.linspace(-8, 8, 1000)\n",
    "\n",
    "M_best = models[np.argmin(AIC)] \n",
    "logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "labels = ['Best-fit Mixture','$\\mathcal{N}(x|0,1)$',\n",
    "          '$\\mathcal{N}(x|-1,1.5)$','$\\mathcal{N}(x|3,0.5)$']\n",
    "\n",
    "plt.hist(X, 100, density=True, histtype='stepfilled', \n",
    "        alpha=0.4,color = 'steelblue',edgecolor = 'black')\n",
    "\n",
    "#Plot the Gaussian mixture\n",
    "plt.plot(x, pdf, label = labels[0])\n",
    "\n",
    "#Plot the individual Gaussians\n",
    "for i, j in enumerate([1,2,3]):\n",
    "    plt.plot(x,pdf_individual[:,i],label = labels[j])\n",
    "    \n",
    "\n",
    "plt.xlabel('$x$', fontsize = 14)\n",
    "plt.ylabel('$p(x)$', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.legend(fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 5))\n",
    "fig.subplots_adjust(left=0.12, right=0.97,\n",
    "                    bottom=0.21, top=0.9, wspace=0.5)\n",
    "\n",
    "# plot 2: AIC and BIC\n",
    "ax = fig.add_subplot(132)\n",
    "\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, '--k', label='BIC')\n",
    "\n",
    "ax.set_xticks(np.arange(1,10,1))\n",
    "ax.set_xlabel('n. components')\n",
    "ax.set_ylabel('information criterion')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "# plot 3: posterior probabilities for each component\n",
    "ax = fig.add_subplot(133)\n",
    "\n",
    "p = responsibilities\n",
    "p = p[:, (1, 0, 2)]  # rearrange order so the plot looks better\n",
    "p = p.cumsum(1).T\n",
    "\n",
    "ax.fill_between(x, 0, p[0], color='gray', alpha=0.3)\n",
    "ax.fill_between(x, p[0], p[1], color='gray', alpha=0.5)\n",
    "ax.fill_between(x, p[1], 1, color='gray', alpha=0.7)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(r'$p({\\rm class}|x)$')\n",
    "\n",
    "ax.text(-5, 0.3, 'class 1', rotation='vertical')\n",
    "ax.text(0, 0.5, 'class 2', rotation='vertical')\n",
    "ax.text(3, 0.3, 'class 3', rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len = [0,1,2]\n",
    "means = [(round(float(models[2].means_[i]),3)) for i in len]\n",
    "covariances = [(round(float(models[2].covariances_[i]),3)) for i in len]\n",
    "weights = [(round(float(models[2].weights_[i]),3)) for i in len]\n",
    "\n",
    "print(f\"\"\"means = {means} \n",
    "covariances = {covariances}\n",
    "weights = {weights}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence estimation: Bootstrap and Jackknife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.resample import bootstrap\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "m = 1000  # number of points\n",
    "n = 10000  # number of bootstraps\n",
    "\n",
    "# sample values from a normal distribution\n",
    "np.random.seed(123)\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "# Compute bootstrap resamplings of data\n",
    "mu1_bootstrap = bootstrap(data, n,  np.std, kwargs=dict(axis=1, ddof=1))\n",
    "mu2_bootstrap = bootstrap(data, n, sigmaG, kwargs=dict(axis=1))\n",
    "\n",
    "# # Compute the theoretical expectations for the two distributions\n",
    "x = np.linspace(0.8, 1.2, 1000)\n",
    "\n",
    "sigma1 = 1. / np.sqrt(2 * (m - 1))\n",
    "pdf1 = norm(1, sigma1).pdf(x)\n",
    "\n",
    "sigma2 = 1.06 / np.sqrt(m) \n",
    "pdf2 = norm(1, sigma2).pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.hist(mu1_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std. dev.)}$')\n",
    "ax.plot(x, pdf1, color='gray')\n",
    "\n",
    "ax.hist(mu2_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$')\n",
    "ax.plot(x, pdf2, color='gray')\n",
    "\n",
    "ax.set_xlim(0.82, 1.18)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$', fontsize = 16)\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$', fontsize = 16)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jackknife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.resample import jackknife\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "np.random.seed(123)\n",
    "m = 1000\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "# Compute jackknife resampling\n",
    "\n",
    "# Standard deviation based\n",
    "mu_s, sigma_mu_s, mu_s_raw = jackknife(data, np.std,\n",
    "                                    kwargs=dict(axis=1, ddof=1),\n",
    "                                    return_raw_distribution=True)\n",
    "\n",
    "pdf1_theory = norm(1, 1. / np.sqrt(2 * (m - 1)))\n",
    "pdf1_jackknife = norm(mu_s, sigma_mu_s)\n",
    "\n",
    "# Sigma_G based\n",
    "mu_sigG, sigma_mu_sigG, mu_sigG_raw = jackknife(data, sigmaG,\n",
    "                                    kwargs=dict(axis=1),\n",
    "                                    return_raw_distribution=True)\n",
    "pdf2_theory = norm(data.std(), 1.06 / np.sqrt(m))\n",
    "pdf2_jackknife = norm(mu_sigG, sigma_mu_sigG)\n",
    "\n",
    "\n",
    "print(f\"mu_s = {mu_s:.3}, sigma_mu_s = {sigma_mu_s:.3}\")\n",
    "print(f\"mu_sigmaG = {mu_sigG:.3}, sigma_mu_sigmaG = {sigma_mu_sigG:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.subplots_adjust(left=0.11, right=0.95, bottom=0.2, top=0.9,\n",
    "                    wspace=0.25)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.hist(mu_s_raw, np.linspace(0.996, 1.008, 100),\n",
    "        label=r'$\\sigma^*\\ {\\rm (std.\\ dev.)}$',\n",
    "        histtype='stepfilled', fc='white', ec='black', density=False)\n",
    "ax.hist(mu_sigG_raw, np.linspace(0.996, 1.008, 100),\n",
    "        label=r'$\\sigma_G^*\\ {\\rm (quartile)}$',\n",
    "        histtype='stepfilled', fc='gray', density=False)\n",
    "ax.legend(loc='upper left', handlelength=2, fontsize = 14)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.004))\n",
    "ax.set_xlabel(r'$\\sigma^*$', fontsize = 14)\n",
    "ax.set_ylabel(r'$N(\\sigma^*)$', fontsize = 14)\n",
    "ax.set_xlim(0.998, 1.008)\n",
    "ax.set_ylim(0, 550)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "x = np.linspace(0.45, 1.15, 1000)\n",
    "ax.plot(x, pdf1_jackknife.pdf(x),\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std.\\ dev.)}$',\n",
    "        zorder=2)\n",
    "ax.plot(x, pdf1_theory.pdf(x), color='gray', zorder=1)\n",
    "\n",
    "ax.plot(x, pdf2_jackknife.pdf(x),\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$', zorder=2)\n",
    "ax.plot(x, pdf2_theory.pdf(x), color='gray', zorder=1)\n",
    "plt.legend(loc='upper left', handlelength=2, fontsize = 14)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$', fontsize = 14)\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$', fontsize = 14)\n",
    "ax.set_xlim(0.45, 1.15)\n",
    "ax.set_ylim(0, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
