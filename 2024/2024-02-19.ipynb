{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class notebook: 2024-02-19\n",
    "\n",
    "In this notebook, we will look at a few different regression techniques. We demonstrate this on a simulated supernova distance modulus-redshift dataset.  \n",
    "\n",
    "This notebook is intended to support Chapter 8 of the textbook, and material is taken from the following script (from astroML):\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter8/astroml_chapter8_Regression.ipynb\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter8/fig_regression_mu_z.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter8/fig_rbf_ridge_mu_z.py\n",
    "* https://github.com/astroML/astroML_figures/blob/main/book_figures/chapter8/fig_gp_mu_z.py\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter8/astroml_chapter8_Regression_with_Errors_on_Dependent_and_Independent_Variables.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first create some fake data with error bars from a true underlying model\n",
    "\n",
    "from astropy.cosmology import LambdaCDM\n",
    "from astroML.datasets import generate_mu_z\n",
    "# this calls an inbuild-function here https://github.com/astroML/astroML/blob/main/astroML/datasets/generated.py\n",
    "# the error bars have the form dmu = dmu_0 + dmu_1 * mu\n",
    "\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0, dmu_0=0.1, dmu_1=0.02)\n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = cosmo.distmod(z)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We first try a linear regression using the matrix formalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mu_sample.reshape(100,1)\n",
    "M = np.zeros((100,2))\n",
    "M[:,0] = 1\n",
    "M[:,1] = z_sample.copy()\n",
    "C = np.diag(np.ones(100)*dmu**2)\n",
    "# C = np.diag(np.ones(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = np.linalg.inv(np.dot(np.dot(M.T, np.linalg.inv(C)), M))\n",
    "Theta2 = np.dot(np.dot(M.T, np.linalg.inv(C)), Y)\n",
    "Theta = np.dot(Theta1, Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta, Theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "plt.plot(z, Theta[0]+Theta[1]*z, ls='--', color='grey')\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We next move onto the inbuilt functions in sklearn, still linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LinearRegression_sk \n",
    "\n",
    "linear_sk = LinearRegression_sk()\n",
    "linear_sk.fit(z_sample[:,None], mu_sample)\n",
    "\n",
    "mu_fit_sk = linear_sk.predict(z[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "plt.plot(z, Theta[0]+Theta[1]*z, ls='--', color='b')\n",
    "# note that in the above we have not considered errors, which is resulting in \n",
    "# some difference compared with the matrix formalism\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(z_sample[:,None], mu_sample, dmu)\n",
    "# now including the errors\n",
    "\n",
    "mu_fit = linear.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_fit, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "plt.plot(z, Theta[0]+Theta[1]*z, ls='--', color='b')\n",
    "\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next try basis function regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poynomials\n",
    "\n",
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "# 2nd degree polynomial regression\n",
    "polynomial = PolynomialRegression(degree=2)\n",
    "polynomial.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_poly = polynomial.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_poly, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian basis\n",
    "\n",
    "from astroML.linear_model import BasisFunctionRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our number of Gaussians\n",
    "nGaussians = 10\n",
    "basis_mu = np.linspace(0, 2, nGaussians)[:, None]\n",
    "basis_sigma = 3 * (basis_mu[1] - basis_mu[0])\n",
    "\n",
    "gauss_basis = BasisFunctionRegression('gaussian', mu=basis_mu, sigma=basis_sigma)\n",
    "gauss_basis.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_gauss = gauss_basis.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_gauss, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try kernal regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian kernal (Nadaraya-Watson)\n",
    "\n",
    "from astroML.linear_model import NadarayaWatson\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define kernal\n",
    "\n",
    "NWkernal = NadarayaWatson('gaussian', h=0.1)\n",
    "\n",
    "NWkernal.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_NWkernal = NWkernal.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_NWkernal, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at some regularized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no pregression, Ridge regression (L2), Lasso regression (L1)\n",
    "\n",
    "regularization = ['none', 'l2', 'l1']\n",
    "kwargs = [dict(), dict(alpha=0.005), dict(alpha=0.001)]\n",
    "labels = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "color= ['teal', 'orange', 'navy']\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "# Manually convert data to a gaussian basis\n",
    "#  note that we're ignoring errors here, for the sake of example.\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "\n",
    "centers = np.linspace(0, 1.8, 100)\n",
    "widths = 0.2\n",
    "X = gaussian_basis(z_sample[:, np.newaxis], centers, widths)\n",
    "\n",
    "for i in range(3):\n",
    "    clf = LinearRegression(regularization=regularization[i],\n",
    "                           fit_intercept=True, kwds=kwargs[i])\n",
    "    clf.fit(X, mu_sample, dmu)\n",
    "    fit = clf.predict(gaussian_basis(z[:, None], centers, widths))\n",
    "\n",
    "    ax.plot(z, fit, '-k', color=color[i], label=labels[i])\n",
    "\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Gaussian process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# fit the data\n",
    "# Mesh the input space for evaluations of the real function,\n",
    "# the prediction and its MSE\n",
    "z_fit = np.linspace(0, 2, 1000)\n",
    "\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu ** 2)\n",
    "\n",
    "gp.fit(z_sample[:, None], mu_sample)\n",
    "y_pred, sigma = gp.predict(z_fit[:, None], return_std=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot the gaussian process\n",
    "#  gaussian process allows computation of the error at each point\n",
    "#  so we will show this as a shaded region\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_true, '--k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', markersize=6)\n",
    "ax.plot(z_fit, y_pred, '-k')\n",
    "ax.fill_between(z_fit, y_pred - 1.96 * sigma, y_pred + 1.96 * sigma,\n",
    "                alpha=0.2, color='b', label='95% confidence interval')\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(36, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now nonlinear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.utils.decorators import pickle_results\n",
    "\n",
    "# might want to use tighter errors here\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0, dmu_0=0.001, dmu_1=0.001)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# define a log likelihood in terms of the parameters\n",
    "#  beta = [omegaM, omegaL]\n",
    "def compute_logL(beta):\n",
    "    cosmo = LambdaCDM(H0=71, Om0=beta[0], Ode0=beta[1], Tcmb0=0)\n",
    "    mu_pred = cosmo.distmod(z_sample).value\n",
    "    return - np.sum(0.5 * ((mu_sample - mu_pred) / dmu) ** 2)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define a function to compute (and save to file) the log-likelihood\n",
    "@pickle_results('mu_z_nonlinear.pkl')\n",
    "def compute_mu_z_nonlinear(Nbins=200):\n",
    "    omegaM = np.linspace(0.05, 0.75, Nbins)\n",
    "    omegaL = np.linspace(0.4, 1.1, Nbins)\n",
    "\n",
    "    logL = np.empty((Nbins, Nbins))\n",
    "\n",
    "    for i in range(len(omegaM)):\n",
    "        for j in range(len(omegaL)):\n",
    "            logL[i, j] = compute_logL([omegaM[i], omegaL[j]])\n",
    "\n",
    "    return omegaM, omegaL, logL\n",
    "\n",
    "\n",
    "omegaM, omegaL, res = compute_mu_z_nonlinear()\n",
    "res -= np.max(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# left plot: the data and best-fit\n",
    "ax = fig.add_subplot(121)\n",
    "whr = np.where(res == np.max(res))\n",
    "omegaM_best = omegaM[whr[0][0]]\n",
    "omegaL_best = omegaL[whr[1][0]]\n",
    "cosmo = LambdaCDM(H0=71, Om0=omegaM_best, Ode0=omegaL_best, Tcmb0=0)\n",
    "\n",
    "z_fit = np.linspace(0.04, 2, 100)\n",
    "mu_fit = cosmo.distmod(z_fit).value\n",
    "\n",
    "ax.plot(z_fit, mu_fit, '-k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray')\n",
    "\n",
    "ax.set_xlim(0, 1.8)\n",
    "ax.set_ylim(36, 46)\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.text(0.04, 0.96, \"%i observations\" % len(z_sample),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "# right plot: the likelihood\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "ax.contour(omegaM, omegaL, convert_to_stdev(res.T),\n",
    "           levels=(0.683, 0.955, 0.997),\n",
    "           colors='k')\n",
    "\n",
    "ax.plot([0, 1], [1, 0], '--k')\n",
    "ax.plot([0, 1], [0.73, 0.73], ':k')\n",
    "ax.plot([0.27, 0.27], [0, 2], ':k')\n",
    "\n",
    "ax.set_xlim(0.05, 0.75)\n",
    "ax.set_ylim(0.4, 1.1)\n",
    "\n",
    "ax.set_xlabel(r'$\\Omega_M$')\n",
    "ax.set_ylabel(r'$\\Omega_\\Lambda$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do a MCMC here too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with errors on both dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.datasets import simulation_kelly\n",
    "\n",
    "\n",
    "ksi_0 = np.arange(np.min(ksi) - 0.5, np.max(ksi) + 0.5)\n",
    "eta_0 = alpha_in + ksi_0 * beta_in\n",
    "\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "figure.subplots_adjust(left=0.1, right=0.95,\n",
    "                       bottom=0.1, top=0.95,\n",
    "                       hspace=0.1, wspace=0.15)\n",
    "ax = figure.add_subplot(221)\n",
    "ax.scatter(ksi, eta)\n",
    "ax.set_xlabel(r'$\\xi$')\n",
    "ax.set_ylabel(r'$\\eta$')\n",
    "\n",
    "ax.plot(ksi_0, eta_0, color='orange')\n",
    "\n",
    "for scalex, scaley, axn in [(0.1, 0.1, 222), (0.3, 0.5, 224), (0.2, 0.2, 223)]:\n",
    "    _, _, xi, yi, xi_error, yi_error, _, _ = simulation_kelly(size=100, scalex=scalex, scaley=scaley,\n",
    "                                                              alpha=alpha_in, beta=beta_in, ksi=ksi, eta=eta)    \n",
    "    ax = figure.add_subplot(axn)\n",
    "\n",
    "    ax.scatter(xi[0], yi, alpha=0.5)\n",
    "    ax.errorbar(xi[0], yi, xerr=xi_error[0], yerr=yi_error, alpha=0.3, ls='')\n",
    "    ax.set_xlabel(f'x, error scaler: {scalex}')\n",
    "    ax.set_ylabel(f'y, error scaler: {scaley}')\n",
    "\n",
    "    x0 = np.arange(np.min(xi) - 0.5, np.max(xi) + 0.5)\n",
    "    y0 = alpha_in + x0 * beta_in\n",
    "    ax.plot(x0, y0, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import LinearRegressionwithErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_xy_err = LinearRegressionwithErrors()\n",
    "linreg_xy_err.fit(xi, yi, yi_error, xi_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.plotting import plot_regressions, plot_regression_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regressions(ksi, eta, xi[0], yi, xi_error[0], yi_error, add_regression_lines=True, alpha_in=alpha_in, beta_in=beta_in)\n",
    "plot_regression_from_trace(linreg_xy_err, (xi, yi, xi_error, yi_error), ax=plt.gca(), chains=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
