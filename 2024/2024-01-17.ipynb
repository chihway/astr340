{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class notebook: 2024-01-17\n",
    "\n",
    "In this notebook, we will look at some common usages of classical statistical inference. We first look at the maximum likelihood estimator (MLE), then look at various applications of MLE. \n",
    "\n",
    "This notebook is intended to support Chapter 4.1-4.5 of the textbook, and material is taken from the following scripts (from astroML):\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Maximum_Likelihood_Estimation_and_Goodness_of_fit.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Gaussian_mixtures.ipynb\n",
    "* https://github.com/astroML/astroML-notebooks/blob/main/chapter4/astroml_chapter4_Confidence_estimates.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator (MLE) and Goodness-of-fit\n",
    "\n",
    "Let's walk through an example. \n",
    "\n",
    "We have 120 measurements for the apparent magnitude of our star drawn from a Gaussian distribution with $\\mu = 8$ (mean apparent magnitude) and $\\sigma = 2$ (the error in our measurements). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "Nsamples= 120\n",
    "mu_true = 8\n",
    "x = np.linspace(0, 16, 1000)\n",
    "\n",
    "# draw 120 pints from a Gaussian with mu = 8 and sigma = 2\n",
    "measurements = np.random.normal(mu_true, 2, Nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(measurements, bins='fd', density=True)\n",
    "plt.plot(x, norm(8, 2).pdf(x))\n",
    "plt.xlabel('mag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate likelihood for 1000 possible values of mu from 0 to 16\n",
    "products = []\n",
    "for i in x:\n",
    "    j = np.prod(norm(measurements, 2).pdf(i))\n",
    "    # this is a neat way for coding this up!\n",
    "    products.append(j)\n",
    "\n",
    "plt.plot(x,products)\n",
    "plt.xlim(0, 16)\n",
    "plt.xlabel('$\\mu$', fontsize = 14)\n",
    "plt.ylabel(r'$L(\\mu)$',fontsize = 14);\n",
    "\n",
    "\n",
    "print(f\"\"\"Mean of our dataset: {np.mean(measurements):.2}\n",
    "The value of mu that maximizes L: {x[products.index(max(products))]:.2}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, the value of $\\mu$, which maximizes $L$, is the average of our measurements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLE in the case of truncated and censored data\n",
    "\n",
    "Here's another example.\n",
    "\n",
    "On the planet of Caladan, meteorites frequently hit the surface. Suppose some collectors are creating a museum of large meteorites, which they've defined as any meteorite larger than 45 meters in diameter. Before they get entered into the museum, they are held in a back room so a new exhibit can be set up. We also know that on this planet, meteorites are normally distributed, centered at 25 meters, with a standard deviation of 13. Suppose you were to enter the back room and find a meteorite of diameter $S_d$. What is your best estimate for the mean diameter of the meteorites that land on Caladan if you knew that the standard deviation is 13 but didn't know the mean was 25?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.stats import norm\n",
    "\n",
    "fig = plt.subplots(figsize=(12, 7.5))\n",
    "\n",
    "x = np.linspace(44, 75, 1000)\n",
    "mean = 25\n",
    "std = 13\n",
    "x_min = 45\n",
    "x_max = np.inf\n",
    "a, b = (x_min - mean) / std, (x_max - mean) / std\n",
    "r = truncnorm.rvs(a, b, loc = mean, scale = std, size=1000)\n",
    "plt.plot(x, truncnorm.pdf(x,a,b,loc = mean, scale = std));\n",
    "plt.hist(r, bins = 'fd', density = True);\n",
    "\n",
    "# this is the true mean of the population\n",
    "print(f'Mean = {np.mean(r):.3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the negative log likelihood\n",
    "# nominally this is just for one meteorite \n",
    "# but we add a second term for illustration\n",
    "\n",
    "const = (-1/2)*(np.log(2*np.pi*(13**2)))\n",
    "\n",
    "def neg_log_likelihood(mu, x_1, x_2=0, n=1, sum_term_x2=0):\n",
    "    sum_term_x1 = ((x_1 - mu)**2)/(2*(13**2))\n",
    "    \n",
    "    if x_2 != 0:\n",
    "        sum_term_x2 = ((x_2 - mu)**2)/(2*(13**2))\n",
    "    \n",
    "    norm_constant = np.log((norm(mu,13).cdf(x_max) - (norm(mu,13).cdf(x_min)))**(-1))\n",
    "    logL = -(const - (sum_term_x1+sum_term_x2)+(n*norm_constant))\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value in args is x_1m the value right after neg_log_likelihood\n",
    "# is the initial value of the maxlike\n",
    "\n",
    "print(f\"\"\"Value of μ which maximizes lnL for:\n",
    "\n",
    "S_d = 49: {minimize(neg_log_likelihood,30,args = (49)).get(\"x\")[0]}\n",
    "S_d = 52: {minimize(neg_log_likelihood,30,args = (52)).get(\"x\")[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we see two meteorites?\n",
    "\n",
    "print(f\"\"\"Values of μ which maximizes lnL for:\n",
    "\n",
    "S_d1 = 49, S_d2 = 50 (both meteorites with diameters below the mean): {minimize(neg_log_likelihood,30,args = (49,50,2)).get(\"x\")[0]}\n",
    "S_d1 = 50, S_d2 = 51 (one above the mean, one below the mean): {minimize(neg_log_likelihood,30,args = (50,51,2)).get(\"x\")[0]}\n",
    "S_d1 = 52, S_d2 = 53 (both meteorites with diameters above the mean): {minimize(neg_log_likelihood,30,args = (52,53,2)).get(\"x\")[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into a large meteorite, do not automatically assume that all meteorites have large diameters on average because it could be due to selection effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goodness-of-Fit and model selection\n",
    "\n",
    "We draw 10 samples from a Gaussian and a Poisson distribution with the same mu, and we are assuming sigma is known. From the two dataset we estimate mu, together with sigma we can fully specify a Gaussian distribution. We ask which data does the Gaussian model fits better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "N_samples = 10\n",
    "mu = 25\n",
    "sigma = 13\n",
    "\n",
    "## Gaussian data on Gaussian model\n",
    "x_gauss = np.random.normal(mu, sigma, N_samples)\n",
    "mu_gauss = np.average(x_gauss)\n",
    "chi_2 = np.sum(np.square((x_gauss - mu_gauss) / sigma))\n",
    "print(chi_2)\n",
    "\n",
    "## Poisson data on Gaussian model\n",
    "x_poisson = np.random.poisson(mu, Nsamples)\n",
    "mu_poisson = np.average(x_poisson)\n",
    "chi_2_poisson = np.sum(np.square((x_poisson - mu_poisson) / sigma))\n",
    "print(chi_2_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.subplots(figsize=(12, 7.5))\n",
    "z = np.linspace(0,32,1000)\n",
    "\n",
    "plt.plot(z, chi2.pdf(z, N_samples - 1))\n",
    "plt.axvline(chi_2, color='green',label = f\"Correct model = {chi_2:.3}\");\n",
    "plt.axvline(chi_2_poisson, color='red',label = f\"Incorrect model ={chi_2_poisson:.3}\");\n",
    "\n",
    "plt.ylim(0,0.12)\n",
    "plt.legend(loc = \"upper right\", fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\chi^2$ value drawn from a Poisson distribution is less likely to occur, whereas the $\\chi^2$ value drawn from the Gaussian is more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goodness-of-Fit and model selection: a more practical example \n",
    "\n",
    "Consider the simple case of the luminosity of a single star being measured multiple times. Our model is that of a star with **no intrinsic luminosity variation**.\n",
    "\n",
    "We will examine four different scenarios:\n",
    "- Correct model with correct errors\n",
    "- Correct model with overestimated errors\n",
    "- Correct model with underestimated errors\n",
    "- Incorrect model with correct errors\n",
    "\n",
    "First, we will define $N$, $\\ell^0$ (the constant luminosity of our star), and $\\sigma_\\ell$ (the measurement error).\n",
    "\n",
    "our models, and our errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = 50\n",
    "luminosity = 10\n",
    "sigma_L = 0.2\n",
    "\n",
    "t = np.linspace(0, 1, N)\n",
    "L_obs = np.random.normal(luminosity, sigma_L, N)\n",
    "\n",
    "y_vals = [L_obs, L_obs, L_obs, L_obs + 0.5 - t ** 2]\n",
    "y_errs = [sigma_L, sigma_L * 2, sigma_L / 2, sigma_L]\n",
    "titles = ['correct errors',\n",
    "          'overestimated errors',\n",
    "          'underestimated errors',\n",
    "          'incorrect model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2, 2, 1 + i, xticks=[])\n",
    "\n",
    "    # compute the mean and the chi^2/dof\n",
    "    mu = np.mean(y_vals[i])\n",
    "    z = (y_vals[i] - mu) / y_errs[i]\n",
    "    chi2 = np.sum(z ** 2)\n",
    "    chi2dof = chi2 / (N - 1)\n",
    "\n",
    "    # compute the standard deviations of chi^2/dof\n",
    "    sigma = np.sqrt(2. / (N - 1))\n",
    "    nsig = (chi2dof - 1) / sigma\n",
    "\n",
    "    # plot the points with errorbars\n",
    "    ax.errorbar(t, y_vals[i], y_errs[i],fmt='.k', ecolor='gray', lw=1,ms = 12)\n",
    "    ax.plot([-0.1, 1.3], [luminosity, luminosity], ':k', lw=3)\n",
    "\n",
    "    # Add labels and text\n",
    "    ax.text(0.95, 0.95, titles[i], ha='right', va='top',\n",
    "            transform=ax.transAxes,fontsize = 15,\n",
    "            bbox=dict(boxstyle='round', fc='w', ec='k'))\n",
    "    ax.text(0.02, 0.02, r'$\\hat{\\mu} = %.2f$' % mu, ha='left', va='bottom',\n",
    "            transform=ax.transAxes, fontsize = 15)\n",
    "    ax.text(0.98, 0.02, r'$\\chi^2_{\\rm dof} = %.2f\\, (%.2g\\,\\sigma)$'\n",
    "            % (chi2dof, nsig), ha='right', va='bottom', \n",
    "            transform=ax.transAxes,fontsize = 15)\n",
    "\n",
    "    # set axis limits\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_ylim(8.6, 11.4)\n",
    "\n",
    "    # set ticks and labels\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    if i > 1: ax.set_xlabel('observations', fontsize = 18)\n",
    "    if i % 2 == 0: ax.set_ylabel('Luminosity',fontsize = 18)\n",
    "    else: ax.yaxis.set_major_formatter(plt.NullFormatter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\chi^2_{\\text{dof}} \\approx 1$ indicates that the model fits the data well (upper-left panel). $\\chi^2_{\\text{dof}}$ much smaller than 1 (upper-right panel) is an indication that the errors are overestimated. $\\chi^2_{\\text{dof}}$ much larger than 1 is an indication either that the errors are underestimated (lower-left panel) or that the model is not a good description of the data (lower-right panel). In this last case, it is clear from the data that the star’s luminosity is varying with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE with Gaussian Mixture Model\n",
    "\n",
    "\n",
    "Imagine we have a Gaussian mixture of distributions $\\mathcal{N}(-1,1.5)$, $\\mathcal{N}(0,1)$, and $\\mathcal{N}(3,0.5)$. In this case, $M$ = 3 (number of separate Gaussian distributions), $\\boldsymbol{\\theta}$ includes the normalization factors for each distribution, $\\alpha_1$,$\\alpha_2$,and $\\alpha_3$ as well as the descriptive parameters $\\mu_1$,$\\sigma_1$,$\\mu_2$,$\\sigma_2$,and $\\mu_3$,$\\sigma_3$\n",
    "\n",
    "\n",
    "First we will define our distributions and combine them using `numpy.concatenate`. Then we will create models using `sklearn.mixture.GaussianMixture`that range from one class to ten classes and calculate the AIC and BIC to find the optimal number of classes for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "random_state = np.random.RandomState(seed=1)\n",
    "\n",
    "X = np.concatenate([random_state.normal(-1, 1.5, 350),\n",
    "                    random_state.normal(0, 1, 500),\n",
    "                    random_state.normal(3, 0.5, 150)]).reshape(-1, 1)\n",
    "\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot our results. By using `np.argmin` on our AIC and BIC arrays, we can find the model with the most optimal  $M$ value. After this, we will use `.score_samples` on this model to compute the log-likelihood (the PDF of the sum of Gaussians). Then we can use `.predict_proba` on our log-likelihood to get the density of the $j$th component.\n",
    "\n",
    "Afterward, we can plot `pdf`, our Gaussian mixture, and `pdf_individual` for the three separate Gaussians along with the histogram of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "x = np.linspace(-8, 8, 1000)\n",
    "\n",
    "M_best = models[np.argmin(AIC)] \n",
    "logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "labels = ['Best-fit Mixture','$\\mathcal{N}(x|0,1)$',\n",
    "          '$\\mathcal{N}(x|-1,1.5)$','$\\mathcal{N}(x|3,0.5)$']\n",
    "\n",
    "plt.hist(X, 100, density=True, histtype='stepfilled', \n",
    "        alpha=0.4,color = 'steelblue',edgecolor = 'black')\n",
    "\n",
    "#Plot the Gaussian mixture\n",
    "plt.plot(x, pdf, label = labels[0])\n",
    "\n",
    "#Plot the individual Gaussians\n",
    "for i, j in enumerate([1,2,3]):\n",
    "    plt.plot(x,pdf_individual[:,i],label = labels[j])\n",
    "    \n",
    "\n",
    "plt.xlabel('$x$', fontsize = 14)\n",
    "plt.ylabel('$p(x)$', fontsize = 14)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.legend(fontsize = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(N, AIC, '-k', label='AIC')\n",
    "plt.plot(N, BIC, '--k', label='BIC')\n",
    "\n",
    "plt.xticks(np.arange(1,10,1))\n",
    "plt.xlabel('n. components')\n",
    "plt.ylabel('information criterion')\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence estimation: Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from astroML.resample import bootstrap\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "m = 1000  # number of points\n",
    "n = 10000  # number of bootstraps\n",
    "\n",
    "# sample values from a normal distribution\n",
    "np.random.seed(123)\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "# Compute bootstrap resamplings of data\n",
    "mu1_bootstrap = bootstrap(data, n,  np.std, kwargs=dict(axis=1, ddof=1))\n",
    "mu2_bootstrap = bootstrap(data, n, sigmaG, kwargs=dict(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the theoretical expectations for the two distributions\n",
    "x = np.linspace(0.8, 1.2, 1000)\n",
    "\n",
    "# error on the estimation of sigma from bootstrap\n",
    "sigma1 = 1. / np.sqrt(2 * (m - 1))\n",
    "pdf1 = norm(1, sigma1).pdf(x)\n",
    "\n",
    "# error on the estimation of sigma from bootstrap\n",
    "sigma2 = 1.06 / np.sqrt(m) \n",
    "pdf2 = norm(1, sigma2).pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.hist(mu1_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std. dev.)}$')\n",
    "ax.plot(x, pdf1, color='gray')\n",
    "\n",
    "ax.hist(mu2_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$')\n",
    "ax.plot(x, pdf2, color='gray')\n",
    "\n",
    "ax.set_xlim(0.82, 1.18)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$', fontsize = 16)\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$', fontsize = 16)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence estimation: Jackknife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.resample import jackknife\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "np.random.seed(123)\n",
    "m = 1000\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "# Compute jackknife resampling\n",
    "\n",
    "# Standard deviation based\n",
    "mu_s, sigma_mu_s, mu_s_raw = jackknife(data, np.std,\n",
    "                                    kwargs=dict(axis=1, ddof=1),\n",
    "                                    return_raw_distribution=True)\n",
    "\n",
    "pdf1_theory = norm(1, 1. / np.sqrt(2 * (m - 1)))\n",
    "pdf1_jackknife = norm(mu_s, sigma_mu_s)\n",
    "\n",
    "# Sigma_G based\n",
    "mu_sigG, sigma_mu_sigG, mu_sigG_raw = jackknife(data, sigmaG,\n",
    "                                    kwargs=dict(axis=1),\n",
    "                                    return_raw_distribution=True)\n",
    "pdf2_theory = norm(data.std(), 1.06 / np.sqrt(m))\n",
    "pdf2_jackknife = norm(mu_sigG, sigma_mu_sigG)\n",
    "\n",
    "\n",
    "print(f\"mu_s = {mu_s:.3}, sigma_mu_s = {sigma_mu_s:.3}\")\n",
    "print(f\"mu_sigmaG = {mu_sigG:.3}, sigma_mu_sigmaG = {sigma_mu_sigG:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.subplots_adjust(left=0.11, right=0.95, bottom=0.2, top=0.9,\n",
    "                    wspace=0.25)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.hist(mu_s_raw, np.linspace(0.996, 1.008, 100),\n",
    "        label=r'$\\sigma^*\\ {\\rm (std.\\ dev.)}$',\n",
    "        histtype='stepfilled', fc='white', ec='black', density=False)\n",
    "ax.hist(mu_sigG_raw, np.linspace(0.996, 1.008, 100),\n",
    "        label=r'$\\sigma_G^*\\ {\\rm (quartile)}$',\n",
    "        histtype='stepfilled', fc='gray', density=False)\n",
    "ax.legend(loc='upper left', handlelength=2, fontsize = 14)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.004))\n",
    "ax.set_xlabel(r'$\\sigma^*$', fontsize = 14)\n",
    "ax.set_ylabel(r'$N(\\sigma^*)$', fontsize = 14)\n",
    "ax.set_xlim(0.998, 1.008)\n",
    "ax.set_ylim(0, 550)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "x = np.linspace(0.45, 1.15, 1000)\n",
    "ax.plot(x, pdf1_jackknife.pdf(x),\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std.\\ dev.)}$',\n",
    "        zorder=2)\n",
    "ax.plot(x, pdf1_theory.pdf(x), color='gray', zorder=1)\n",
    "\n",
    "ax.plot(x, pdf2_jackknife.pdf(x),\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$', zorder=2)\n",
    "ax.plot(x, pdf2_theory.pdf(x), color='gray', zorder=1)\n",
    "plt.legend(loc='upper left', handlelength=2, fontsize = 14)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$', fontsize = 14)\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$', fontsize = 14)\n",
    "ax.set_xlim(0.45, 1.15)\n",
    "ax.set_ylim(0, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This failure is a general problem with the standard jackknife method, which performs well for smooth differential statistics such as the mean and standard deviation, but does not perform well for medians, quantiles, and other rank-based statistics. For these sorts of statistics, a jackknife implementation that removes more than one observation can overcome this problem. The reason for this failure becomes apparent upon examination of the figure above: for $\\sigma_G$, the vast majority of jackknife samples yield one of three discrete values! Because quartiles are insensitive to the removal of outliers, all samples created by the removal of a point larger than $q_{75}$ lead to precisely the same estimate. The same is true for removal of any point smaller than $q_{25}$, and for any point in the range $q_{25} < x < q_{75}$. Because of this, the jackknife cannot accurately sample the error distribution, which leads to a gross misestimate of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convinient functions in astropy\n",
    "\n",
    "from astropy.stats import jackknife_stats\n",
    "\n",
    "x = np.random.normal(loc=0, scale=1, size=1000)\n",
    "estimate, bias, stderr, conf_interval = jackknife_stats(x, np.std)\n",
    "print(estimate , stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astr340",
   "language": "python",
   "name": "astr340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
